{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09037afd-1644-4603-9a54-9273828f2974",
   "metadata": {},
   "source": [
    "# Dealing with the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7d1e80-cfdd-40da-bc46-ec2b44db0c7f",
   "metadata": {},
   "source": [
    "## Load HIV dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5369fcc-7cef-4e51-bd2c-5f8a3e350e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e749e01-8963-455e-8f86-154a0ad10153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>activity</th>\n",
       "      <th>HIV_active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCC1=[O+][Cu-3]2([O+]=C(CC)C1)[O+]=C(CC)CC(CC)...</td>\n",
       "      <td>CI</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C(=Cc1ccccc1)C1=[O+][Cu-3]2([O+]=C(C=Cc3ccccc3...</td>\n",
       "      <td>CI</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CC(=O)N1c2ccccc2Sc2c1ccc1ccccc21</td>\n",
       "      <td>CI</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nc1ccc(C=Cc2ccc(N)cc2S(=O)(=O)O)c(S(=O)(=O)O)c1</td>\n",
       "      <td>CI</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O=S(=O)(O)CCS(=O)(=O)O</td>\n",
       "      <td>CI</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              smiles activity  HIV_active\n",
       "0  CCC1=[O+][Cu-3]2([O+]=C(CC)C1)[O+]=C(CC)CC(CC)...       CI           0\n",
       "1  C(=Cc1ccccc1)C1=[O+][Cu-3]2([O+]=C(C=Cc3ccccc3...       CI           0\n",
       "2                   CC(=O)N1c2ccccc2Sc2c1ccc1ccccc21       CI           0\n",
       "3    Nc1ccc(C=Cc2ccc(N)cc2S(=O)(=O)O)c(S(=O)(=O)O)c1       CI           0\n",
       "4                             O=S(=O)(O)CCS(=O)(=O)O       CI           0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"HIV.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cfc6c87-00f9-4e90-8503-8552506bc5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCC1=[O+][Cu-3]2([O+]=C(CC)C1)[O+]=C(CC)CC(CC)=[O+]2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "string = df.loc[0,'smiles']\n",
    "print(df.loc[0,'smiles'])\n",
    "print(string[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf1518a8-a257-475d-9b4d-5759bfa5b5e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "activity\n",
       "CI    39684\n",
       "CM     1039\n",
       "CA      404\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.value_counts(\"activity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "476c8ecc-b806-4989-a409-017a3826e11b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HIV_active\n",
       "0    39684\n",
       "1     1443\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.value_counts(\"HIV_active\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47148457-3ed7-404e-8968-f3cdc2716d74",
   "metadata": {},
   "source": [
    "## Transform into SELFIES representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5850e02-8e28-48ba-9eba-f10edcd1d4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selfies as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f69bc41-fad9-473b-89ad-4f3d97ad378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform\n",
    "df_sf = df.copy()\n",
    "len_sf=[]\n",
    "for ind in range(len(df_sf['smiles'])):\n",
    "    try:\n",
    "       df_sf.loc[ind, 'smiles'] = sf.encoder(df_sf.loc[ind, 'smiles'])\n",
    "    except:\n",
    "        pass #sf.encoder error!\n",
    "\n",
    "    len_sf.append(sf.len_selfies(df_sf.loc[ind, 'smiles']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfad957c-054e-46fd-9393-6af10dead144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selfies</th>\n",
       "      <th>activity</th>\n",
       "      <th>HIV_active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[C][C][C][=O+1][Cu-3][Branch1][#Branch2][O+1][...</td>\n",
       "      <td>CI</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[C][=Branch1][#Branch2][=C][C][=C][C][=C][C][=...</td>\n",
       "      <td>CI</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[C][C][=Branch1][C][=O][N][C][=C][C][=C][C][=C...</td>\n",
       "      <td>CI</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[N][C][=C][C][=C][Branch2][Ring1][=Branch1][C]...</td>\n",
       "      <td>CI</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[O][=S][=Branch1][C][=O][Branch1][C][O][C][C][...</td>\n",
       "      <td>CI</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             selfies activity  HIV_active\n",
       "0  [C][C][C][=O+1][Cu-3][Branch1][#Branch2][O+1][...       CI           0\n",
       "1  [C][=Branch1][#Branch2][=C][C][=C][C][=C][C][=...       CI           0\n",
       "2  [C][C][=Branch1][C][=O][N][C][=C][C][=C][C][=C...       CI           0\n",
       "3  [N][C][=C][C][=C][Branch2][Ring1][=Branch1][C]...       CI           0\n",
       "4  [O][=S][=Branch1][C][=O][Branch1][C][O][C][C][...       CI           0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sf.rename({'smiles':'selfies'}, axis='columns', inplace=True)\n",
    "df_sf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0a4f76f-a92e-4c7c-b7d0-6d1139f05e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][C][C][=O+1][Cu-3][Branch1][#Branch2][O+1][=C][Branch1][Ring1][C][C][C][Ring1][Branch2][O+1][=C][Branch1][Ring1][C][C][C][C][Branch1][Ring1][C][C][=O+1][Ring1][#C]\n"
     ]
    }
   ],
   "source": [
    "string_sf = df_sf.loc[0,'selfies']\n",
    "print(df_sf.loc[0,'selfies'])\n",
    "#print(string_sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f2dacf0-6af4-416d-a8cc-691774292b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.75984146667639\n",
      "21.70874260604909\n"
     ]
    }
   ],
   "source": [
    "# Compute mean and standard deviation\n",
    "print(np.mean(len_sf))\n",
    "print(np.std(len_sf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f35b678-0055-4d8f-bbda-b83134eee82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a histogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(len_sf, bins=range(min(len_sf), max(len_sf) + 1), alpha=0.75, edgecolor='black')\n",
    "plt.title('Histogram of SELFIES String Lengths')\n",
    "plt.xlabel('Length of SELFIES String')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9e7139-05f8-4802-af0b-ecba28a5f961",
   "metadata": {},
   "source": [
    "## Data sampling for the CNN-LSTM Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ea9feeec-2022-47d8-96bb-ea49f3d4d8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the dataframe into three categories\n",
    "df_CI = df_sf[df_sf['activity'] == 'CI']\n",
    "df_CM = df_sf[df_sf['activity'] == 'CM']\n",
    "df_CA = df_sf[df_sf['activity'] == 'CA']\n",
    "\n",
    "# Randomly sample 300 from each category\n",
    "sample_CI = df_CI.sample(n=300, random_state=42)  # replace 1 with your chosen seed for reproducibility\n",
    "sample_CM = df_CM.sample(n=300, random_state=42)\n",
    "sample_CA = df_CA.sample(n=300, random_state=42)\n",
    "\n",
    "# Concatenate the samples back into one DataFrame\n",
    "sampled_sf = pd.concat([sample_CI, sample_CM, sample_CA])\n",
    "\n",
    "# If you want to shuffle the combined DataFrame\n",
    "sampled_sf = sampled_sf.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "98dfcc06-05ae-4a2e-a4e1-0c44fb9158ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selfies</th>\n",
       "      <th>activity</th>\n",
       "      <th>HIV_active</th>\n",
       "      <th>graph_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[C][N][C][=Branch1][C][=O][C][=N][C][=C][N][=C...</td>\n",
       "      <td>CM</td>\n",
       "      <td>1</td>\n",
       "      <td>[(x, [tensor([-0.1129,  1.5156,  1.4165, -0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[O][C][C][N][=C][C][=C][C][=C][N][Ring1][=Bran...</td>\n",
       "      <td>CI</td>\n",
       "      <td>0</td>\n",
       "      <td>[(x, [tensor([-0.1129,  1.5156,  1.4165, -0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[N].[N-1][=N+1][=N+1][=C][C][C][Branch1][=N][N...</td>\n",
       "      <td>CM</td>\n",
       "      <td>1</td>\n",
       "      <td>[(x, [tensor([-0.1129,  1.5156,  1.4165, -0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[C][C][Branch1][C][C][Branch1][C][C][C][=Branc...</td>\n",
       "      <td>CA</td>\n",
       "      <td>1</td>\n",
       "      <td>[(x, [tensor([-0.1129,  1.5156,  1.4165, -0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[O][=C][C][=C][C][=C][N][=C][Ring1][=Branch1][...</td>\n",
       "      <td>CM</td>\n",
       "      <td>1</td>\n",
       "      <td>[(x, [tensor([-0.1129,  1.5156,  1.4165, -0.02...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             selfies activity  HIV_active  \\\n",
       "0  [C][N][C][=Branch1][C][=O][C][=N][C][=C][N][=C...       CM           1   \n",
       "1  [O][C][C][N][=C][C][=C][C][=C][N][Ring1][=Bran...       CI           0   \n",
       "2  [N].[N-1][=N+1][=N+1][=C][C][C][Branch1][=N][N...       CM           1   \n",
       "3  [C][C][Branch1][C][C][Branch1][C][C][C][=Branc...       CA           1   \n",
       "4  [O][=C][C][=C][C][=C][N][=C][Ring1][=Branch1][...       CM           1   \n",
       "\n",
       "                                          graph_data  \n",
       "0  [(x, [tensor([-0.1129,  1.5156,  1.4165, -0.02...  \n",
       "1  [(x, [tensor([-0.1129,  1.5156,  1.4165, -0.02...  \n",
       "2  [(x, [tensor([-0.1129,  1.5156,  1.4165, -0.02...  \n",
       "3  [(x, [tensor([-0.1129,  1.5156,  1.4165, -0.02...  \n",
       "4  [(x, [tensor([-0.1129,  1.5156,  1.4165, -0.02...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_sf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431d66fd-b7e7-4781-8f43-ed755768269e",
   "metadata": {},
   "source": [
    "## Padding, tokenization, and embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "94b83267-6e24-4d2a-a72d-b7b680685ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_selfies(string_sf):\n",
    "    # This regex matches anything inside brackets\n",
    "    selfies_elements = re.findall(r'\\[.*?]', string_sf)\n",
    "    return selfies_elements\n",
    "\n",
    "'''\n",
    "# Example usage:\n",
    "string_sf = df_sf.loc[0, 'selfies']\n",
    "selfies_list = split_selfies(string_sf)\n",
    "print(selfies_list)\n",
    "'''\n",
    "\n",
    "def pad_start_end_token(dataset):\n",
    "    \"\"\"\n",
    "    Pad a list of SMILES with \"SOS\" and \"EOS\" token\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    smiles: list of str\n",
    "        A list containing SMILES strings to pad\n",
    "    \n",
    "    Returns\n",
    "    ------\n",
    "    padded: list of list of str\n",
    "        A list containing padded SMILES strings. Example: [['SOS', 'C', 'EOS'], ...]\n",
    "    \"\"\"\n",
    "    padded = []\n",
    "    for ind in range(len(dataset['selfies'])):\n",
    "        padded.append([\"[SOS]\"] + split_selfies(df_sf.loc[ind, 'selfies']) + [\"[EOS]\"])\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "96d2d2ea-3285-419d-8512-4ea020c2b019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[#Branch1]': 0,\n",
       " '[#Branch2]': 1,\n",
       " '[#C]': 2,\n",
       " '[#N]': 3,\n",
       " '[=As]': 4,\n",
       " '[=Branch1]': 5,\n",
       " '[=Branch2]': 6,\n",
       " '[=C]': 7,\n",
       " '[=N+1]': 8,\n",
       " '[=N]': 9,\n",
       " '[=O+1]': 10,\n",
       " '[=O]': 11,\n",
       " '[=P]': 12,\n",
       " '[=Ring1]': 13,\n",
       " '[=Ring2]': 14,\n",
       " '[=S+1]': 15,\n",
       " '[=S]': 16,\n",
       " '[Al]': 17,\n",
       " '[As]': 18,\n",
       " '[B]': 19,\n",
       " '[Br]': 20,\n",
       " '[Branch1]': 21,\n",
       " '[Branch2]': 22,\n",
       " '[C]': 23,\n",
       " '[Cl+3]': 24,\n",
       " '[Cl]': 25,\n",
       " '[Co-4]': 26,\n",
       " '[Cu-3]': 27,\n",
       " '[Cu-5]': 28,\n",
       " '[EOS]': 29,\n",
       " '[F]': 30,\n",
       " '[Fe-2]': 31,\n",
       " '[Fe-3]': 32,\n",
       " '[Fe-4]': 33,\n",
       " '[Hg-1]': 34,\n",
       " '[Hg-2]': 35,\n",
       " '[Hg]': 36,\n",
       " '[I]': 37,\n",
       " '[Li]': 38,\n",
       " '[Mn]': 39,\n",
       " '[N+1]': 40,\n",
       " '[N-1]': 41,\n",
       " '[NH1]': 42,\n",
       " '[N]': 43,\n",
       " '[Ni-2]': 44,\n",
       " '[Ni-4]': 45,\n",
       " '[O+1]': 46,\n",
       " '[O-1]': 47,\n",
       " '[O]': 48,\n",
       " '[PH1]': 49,\n",
       " '[P]': 50,\n",
       " '[Pd-2]': 51,\n",
       " '[Ring1]': 52,\n",
       " '[Ring2]': 53,\n",
       " '[S+1]': 54,\n",
       " '[SOS]': 55,\n",
       " '[S]': 56,\n",
       " '[Sb]': 57,\n",
       " '[Se]': 58,\n",
       " '[SiH1]': 59,\n",
       " '[Si]': 60,\n",
       " '[Sn]': 61,\n",
       " '[V-1]': 62,\n",
       " '[Zn-2]': 63,\n",
       " '[Zr]': 64}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sf = pad_start_end_token(sampled_sf)\n",
    "vocab = {element: idx for idx, element in enumerate(np.unique(np.concatenate(padded_sf)))}\n",
    "vocab\n",
    "#padded_sf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ace65e4b-cfb7-4e83-8402-518238c8ff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_selfies(selfies_list, vocab):\n",
    "    \"\"\"Convert a list of SELFIES elements to a list of indices based on the vocabulary (One-hot encoding).\"\"\"\n",
    "    return [vocab[element] for element in selfies_list if element in vocab]\n",
    "\n",
    "\n",
    "\n",
    "class SelfiesEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SelfiesEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "03aa5c5d-5de2-4850-b5a8-cb33a4e4363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_selfies = [tokenize_selfies(selfies, vocab) for selfies in padded_sf]\n",
    "\n",
    "# Instantiate the embedding layer\n",
    "vocab_size = len(vocab)  # Size of the vocabulary\n",
    "embedding_dim = 64  # The size of each embedding vector, which you can choose\n",
    "'''\n",
    "For the limited gpu resource, set it as 32\n",
    "'''\n",
    "\n",
    "selfies_embedding_layer = SelfiesEmbedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Convert a list of tokenized selfies to a tensor and pass it through the embedding layer\n",
    "# Pad the sequences to the same length to create a tensor\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "padded_tokenized_selfies = pad_sequence([torch.tensor(ts) for ts in tokenized_selfies],\n",
    "                                        batch_first=True,\n",
    "                                        padding_value=0)  # assuming 0 is the padding index\n",
    "\n",
    "# Get embeddings for the batch of tokenized SELFIES\n",
    "embedded_selfies = selfies_embedding_layer(padded_tokenized_selfies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "72553202-b13f-45e7-9e84-09e3653ac87b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i, selfie in enumerate(tokenized_selfies):\\n    print(f\"Length of selfie {i}: {len(selfie)}\")\\n'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before padding\n",
    "\"\"\"\n",
    "for i, selfie in enumerate(tokenized_selfies):\n",
    "    print(f\"Length of selfie {i}: {len(selfie)}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c8635e8c-63c6-43db-b69b-4830dc00baff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The information of SELFIES dataset after tokenized (length of selfies, maximum length of tokenized_selfies, minimum length of tokenized_selfies):\n",
      "900 169 4\n",
      "The information of SELFIES dataset after tokenized (length of selfies, maximum length of padded_tk_selfies, minimum length of padded_tk_selfies):\n",
      "900 169 169\n",
      "torch.Size([900, 169, 64])\n"
     ]
    }
   ],
   "source": [
    "# Find the maximum and minimum length of the tokenized sequences\n",
    "max_length_tk_selfies = max(len(sequence) for sequence in tokenized_selfies)\n",
    "min_length_tk_selfies = min(len(sequence) for sequence in tokenized_selfies)\n",
    "print(f\"The information of SELFIES dataset after tokenized\",\n",
    "      f\"(length of selfies, maximum length of tokenized_selfies, minimum length of tokenized_selfies):\")\n",
    "print(len(tokenized_selfies),max_length_tk_selfies,min_length_tk_selfies)\n",
    "\n",
    "\n",
    "# Find the maximum and minimum length of the tokenized sequences\n",
    "max_length_pt_selfies = max(len(tensor) for tensor in padded_tokenized_selfies)\n",
    "min_length_pt_selfies = min(len(tensor) for tensor in padded_tokenized_selfies)\n",
    "print(f\"The information of SELFIES dataset after tokenized\",\n",
    "      f\"(length of selfies, maximum length of padded_tk_selfies, minimum length of padded_tk_selfies):\")\n",
    "print(len(padded_tokenized_selfies), max_length_pt_selfies, min_length_pt_selfies)\n",
    "\n",
    "print(embedded_selfies.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb3f9de-e04a-4c43-87e5-e69e1b9c4d80",
   "metadata": {},
   "source": [
    "[900, 169, 64] indicates: <br>\n",
    "900: the amount of the dataset <br>\n",
    "169: the largest length of selfies string; after the pad_sequence function, all strings should have the same length which is 169 <br>\n",
    "64: the dimension after the embedding <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352533aa-c1e0-4f06-a8d6-bb60058515cc",
   "metadata": {},
   "source": [
    "**input shape: (n_batch, n_seq_how many data in a batch, input_dim_How many class do you have, features)** <br>\n",
    "**LSTM input: torch.nn.LSTM(self, \n",
    "input_size: input_dim, \n",
    "hidden_size, \n",
    "num_layers: RNN neurons, bias=True, batch_first=False, dropout=0.0, bidirectional=False, proj_size=0, device=None, dtype=None)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaa5360-6f66-4caf-a9b6-ea825f292f78",
   "metadata": {},
   "source": [
    "# CNN-LSTM Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d794dd41-b888-4cf8-8abb-bca5de2e58a8",
   "metadata": {},
   "source": [
    "**Reference:** <br>\n",
    "Convolutional, Long Short-Term Memory, Fully Connected Deep Neural Networks. IEEE Conference Publication | IEEE Xplore, 1 Apr. 2015, ieeexplore.ieee.org/document/7178838/authors#authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cbaf938e-4389-4ef1-99a9-2a8511072ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class SelfiesDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        # Assuming data is a 3D tensor of shape (batch_size, sequence_length, embedding_dim)\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Assuming that the last dimension is the features/embedding dimension\n",
    "        x = self.data[idx, :-1, :]  # All but the last feature. Use LSTM to predict the last position of series strings\n",
    "        y = self.data[idx, 1:, :]   # All but the first feature (assuming you're predicting the next feature)\n",
    "        #print(f\"the shape of X:\", x.shape)\n",
    "        #print(f\"the shape of y:\", y.shape)\n",
    "        return x,y\n",
    "#data_ini = SelfiesDataset(embedded_selfies) # initialize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "793b30f9-fd0f-4fc0-aa74-351d81b784a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class SelfiesDataset_ori(Dataset):\n",
    "    def __init__(self, data):\n",
    "        # Assuming data is a 3D tensor of shape (batch_size, sequence_length, embedding_dim)\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Assuming that the last dimension is the features/embedding dimension\n",
    "        x = self.data[idx, :-1, :]  # All but the last feature\n",
    "        y = self.data[idx, 1:, :]   # All but the first feature (assuming you're predicting the next feature)\n",
    "        return {'X': x, 'y': y}\n",
    "    \n",
    "data_ini = SelfiesDataset_ori(embedded_selfies) # initialize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b81f8690-a945-4021-8399-ba0e0b0cbc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Initialize dataset\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "# Split indices into training and validation sets\n",
    "train_indices, val_indices = train_test_split(range(len(embedded_selfies)), test_size=0.2, random_state=42)\n",
    "\n",
    "# Create two separate datasets\n",
    "train_dataset = torch.utils.data.Subset(SelfiesDataset(embedded_selfies), train_indices)\n",
    "val_dataset = torch.utils.data.Subset(SelfiesDataset(embedded_selfies), val_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3031f4e5-597e-445f-aefb-5ced035df2ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def CNN_out_dim(in_dim, kernel_size, padding, stride, dilation):\n",
    "    return (in_dim + 2 * padding - dilation * (kernel_size - 1) - 1) // stride + 1\n",
    "sequence_length = 169\n",
    "Conv1d_out = CNN_out_dim(sequence_length, kernel_size=5, padding=2, stride=1, dilation=1)\n",
    "Conv1d_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3b3458a1-8e12-499b-9654-f97e72ce593c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, cnn_channels, kernel_size, lstm_hidden_size, sequence_length,lstm_num_layers):\n",
    "        super(CNNLSTMEncoder, self).__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        self.cnn = nn.Conv1d(\n",
    "            in_channels=input_dim, out_channels=cnn_channels,kernel_size=kernel_size, stride=1, padding=2\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=cnn_channels, hidden_size=lstm_hidden_size, num_layers=lstm_num_layers, batch_first=True\n",
    "        )\n",
    "\n",
    "        # Additional layers to get the mean and log variance\n",
    "        actual_length = sequence_length-1\n",
    "        self.mu = nn.Linear(lstm_hidden_size * actual_length, latent_dims)\n",
    "        self.log_var = nn.Linear(lstm_hidden_size * actual_length, latent_dims)\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.transpose(1, 2)  # Reshape to (batch_size, input_dim, sequence_length)\n",
    "        cnn_out = self.relu(self.cnn(x))\n",
    "        cnn_out = cnn_out.transpose(1, 2)  # Reshape to (batch_size, sequence_length, cnn_channels)\n",
    "        lstm_out, _ = self.lstm(cnn_out)\n",
    "        \n",
    "        '''\n",
    "        #For the calculation of dimensionality:\n",
    "        print(f\"sequence_length:\", sequence_length)\n",
    "        print(f\"input shape:\", x.shape)\n",
    "        print(f\"batch size:\", batch_size)\n",
    "        print(f\"input size after transpose:\", x.shape)\n",
    "        print(f\"output size after CNN and relu:\", cnn_out.shape)\n",
    "        print(f\"size before the lstm:\", cnn_out.shape)\n",
    "        print(f\"lstm_out:\", lstm_out.shape)\n",
    "        print(f\"lstm_hidden_size:\", lstm_hidden_size)\n",
    "        print(f\"sequence_length:\", sequence_length)\n",
    "        print(f\"lstm_out_flat size:\", lstm_out_flat.size())  # Debug print statement\n",
    "        print(f\"mu and log_var size:\", mu.shape)\n",
    "        '''\n",
    "        \n",
    "        # Flatten the output of LSTM to connect to fully connected layers\n",
    "        lstm_out_flat = lstm_out.contiguous().view(batch_size, -1)\n",
    "        mu = self.mu(lstm_out_flat).to(device)\n",
    "        log_var = self.log_var(lstm_out_flat).to(device)\n",
    "        return mu, log_var\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.lstm_hidden_size), \n",
    "                torch.zeros(self.num_layers, batch_size, self.lstm_hidden_size))\n",
    "\n",
    "class CNNLSTMDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, cnn_channels, kernel_size, lstm_hidden_size, sequence_length,lstm_num_layers):\n",
    "        super(CNNLSTMDecoder, self).__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=lstm_hidden_size, hidden_size=lstm_hidden_size, num_layers=lstm_num_layers, batch_first=True\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        self.cnn = nn.ConvTranspose1d(\n",
    "            in_channels=lstm_hidden_size, out_channels=input_dim, kernel_size=kernel_size, padding=kernel_size // 2\n",
    "        )\n",
    "        actual_length = sequence_length-1\n",
    "        self.translinear = nn.Linear(latent_dims, lstm_hidden_size * actual_length)\n",
    "        self.unflatten = nn.Unflatten(1, (actual_length, lstm_hidden_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        #print(f\"input shape:\", x.shape)\n",
    "        linear_out = self.translinear(x)\n",
    "        unflatten_out = self.unflatten(linear_out)\n",
    "        lstm_out, _ = self.lstm(unflatten_out)\n",
    "        lstm_out = lstm_out.transpose(1, 2)  # Reshape to (batch_size, lstm_hidden_size, sequence_length)\n",
    "        cnn_out = self.relu(self.cnn(lstm_out))\n",
    "        cnn_out = cnn_out.transpose(1, 2)  # Reshape to (batch_size, sequence_length, input_dim)\n",
    "        #print(cnn_out.shape)\n",
    "        return cnn_out\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.lstm_hidden_size), \n",
    "                torch.zeros(self.num_layers, batch_size, self.lstm_hidden_size))\n",
    "\n",
    "class CNNLSTMVAE(nn.Module):\n",
    "    def __init__(self, input_dim, cnn_channels, kernel_size, lstm_hidden_size, sequence_length, latent_dims, lstm_num_layers):\n",
    "        super(CNNLSTMVAE, self).__init__()\n",
    "        self.encoder = CNNLSTMEncoder(\n",
    "            input_dim, cnn_channels, kernel_size, lstm_hidden_size, sequence_length, lstm_num_layers\n",
    "        )\n",
    "        self.decoder = CNNLSTMDecoder(\n",
    "            input_dim, cnn_channels, kernel_size, lstm_hidden_size, sequence_length, lstm_num_layers\n",
    "        )\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.encoder = self.encoder.to(device)\n",
    "        self.decoder = self.decoder.to(device)\n",
    "        \n",
    "        # Reparameterization trick\n",
    "        self.latent_dims = latent_dims\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var).to(device)\n",
    "        eps = torch.randn_like(std).to(device)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x)\n",
    "        #print(f\"Encoder is completed\",mu.is_cuda, log_var.is_cuda)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        #print(f\"Reparameterize is completed\",z.is_cuda,z.shape)\n",
    "        # The decoder will remain the same, but now it takes 'z' as input\n",
    "        decoded = self.decoder(z)\n",
    "        return decoded, mu, log_var\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.lstm_num_layers, batch_size, self.lstm_hidden_size), \n",
    "                torch.zeros(self.lstm_num_layers, batch_size, self.lstm_hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a9ece943-9197-4280-8318-ece364285f59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "input_dim: Match the embedding dimension\n",
    "cnn_channels: Number of CNN channels\n",
    "kernel_size: Kernel size for the CNN\n",
    "lstm_hidden_size: Hidden size for the LSTM\n",
    "sequence_length: The length of the padded sequences\n",
    "'''\n",
    "\n",
    "input_dim = embedding_dim\n",
    "cnn_channels = 16\n",
    "kernel_size = 5  \n",
    "lstm_hidden_size = 256  \n",
    "sequence_length = 169\n",
    "num_epochs = 10\n",
    "learning_rate=1e-3\n",
    "l2=1e-5\n",
    "latent_dims=256\n",
    "batch_sizes=32\n",
    "lstm_num_layers=1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "CLvae = CNNLSTMVAE(\n",
    "    input_dim, cnn_channels, kernel_size, lstm_hidden_size, sequence_length, latent_dims, lstm_num_layers\n",
    ").to(device)\n",
    "\n",
    "latent_dims = 128  # Dimensionality of the latent representation\n",
    "#CLvae = CNNLSTMVAE(input_dim, cnn_channels, kernel_size, lstm_hidden_size, sequence_length, latent_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c7d7b8ad-7d83-4ada-93cb-ef0404d1a992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def loss_func(outputs, targets, mu, log_var):\n",
    "    reconstruction_loss = F.binary_cross_entropy_with_logits(outputs, targets, reduction='sum')\n",
    "    kl_divergence = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    loss = reconstruction_loss + kl_divergence\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5a48cac8-5aa9-4a99-8309-f68cd21fee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kld_loss_func(mu, sigma):\n",
    "    \"\"\"\n",
    "    KL-Divergence: KLD = 0.5 * sum(\\mu^2 + \\sigma^2 - ln(\\sigma^2) - 1)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mu: torch.Tensor\n",
    "        Mean vector in the VAE bottleneck region\n",
    "    sigma: torch.Tensor\n",
    "        Standard Deviation vector in the VAE bottleneck region\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    kld: torch.Tensor\n",
    "        KL-Divegence loss (a scalar)\n",
    "    \"\"\"\n",
    "    return 0.5 * torch.sum(mu**2 + sigma**2 - torch.log(sigma**2) - 1)\n",
    "\n",
    "\n",
    "def vae_loss_func(recon_x, x, mu, sigma):\n",
    "\n",
    "    bce_loss = econ_loss = F.mse_loss(recon_x, x, reduction='mean')\n",
    "    kld_loss = kld_loss_func(mu, sigma)\n",
    "\n",
    "    return bce_loss + kld_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c82393e7-8908-4265-8c21-9f27983fcc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class VAETrainer:\n",
    "    \n",
    "    def __init__(self, model, learning_rate, batch_size, epoch, l2):\n",
    "        self.model = model\n",
    "        # use cuda if it is available\n",
    "        if torch.cuda.is_available():\n",
    "            model = model.cuda()  #  model\n",
    "            \n",
    "        # calulate the number of parameters    \n",
    "        num_params = sum(item.numel() for item in model.parameters())\n",
    "        print(f\"{model.__class__.__name__} - Number of parameters: {num_params}\")\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2)\n",
    "        \n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "\n",
    "    \n",
    "    def train(self, train_data, val_data, early_stop=True, verbose=True, draw_curve=True):\n",
    "        train_loader = DataLoader(train_data, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        train_loss_list = []\n",
    "        val_loss_list = []\n",
    "        \n",
    "        weights = self.model.state_dict()\n",
    "        lowest_val_loss = np.inf\n",
    "\n",
    "        for n in tqdm(range(self.epoch), leave=False):\n",
    "            self.model.train()\n",
    "            epoch_loss = 0.0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                if torch.cuda.is_available():\n",
    "                    X_batch, y_batch = X_batch.cuda(), y_batch.cuda()\n",
    "\n",
    "                print(X_batch.size)\n",
    "                batch_importance = 1 / len(train_data)\n",
    "                # call the model\n",
    "                hidden = self.model.init_hidden(y_batch.shape[0])\n",
    "                self.model.hidden = self.model.init_hidden(y_batch.shape[0])\n",
    "                X_batch_recon, mu, sigma = self.model(X_batch)\n",
    "                batch_loss = vae_loss_func(X_batch_recon, X_batch, mu, sigma)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                batch_loss.backward(retain_graph=True)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                epoch_loss += batch_loss.detach().cpu().item() * batch_importance\n",
    "            \n",
    "            train_loss_list.append(epoch_loss)\n",
    "            \n",
    "            val_loss = self.evaluate(val_data, print_loss=False)\n",
    "            val_loss_list.append(val_loss)\n",
    "            \n",
    "            if early_stop:\n",
    "                if val_loss < lowest_val_loss:\n",
    "                    lowest_val_loss = val_loss\n",
    "                    weights = self.model.state_dict()\n",
    "            \n",
    "        if draw_curve:\n",
    "            x_axis = np.arange(self.epoch)\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(5, 4))\n",
    "            ax.plot(x_axis, train_loss_list, label=\"Train\")\n",
    "            ax.plot(x_axis, val_loss_list, label=\"Validation\")\n",
    "            ax.set_title(\"Total Loss\")\n",
    "            ax.set_xlabel(\"# Epoch\")\n",
    "        \n",
    "        if early_stop:\n",
    "            self.model.load_state_dict(weights)\n",
    "        \n",
    "        return {\n",
    "            \"train_loss_list\": train_loss_list,\n",
    "            \"val_loss_list\": val_loss_list,\n",
    "        }\n",
    "    \n",
    "    def evaluate(self, data, print_loss=True):\n",
    "        self.model.eval()\n",
    "        loader = DataLoader(data, batch_size=self.batch_size)\n",
    "        total_loss = 0.0\n",
    "        for X_batch, y_batch in loader:\n",
    "            if torch.cuda.is_available():\n",
    "                 X_batch, y_batch = X_batch.cuda(), y_batch.cuda()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                batch_importance = 1 / len(data)\n",
    "                X_batch_recon, mu, sigma = self.model(X_batch)\n",
    "                batch_loss = vae_loss_func(X_batch_recon, X_batch, mu, sigma)\n",
    "                total_loss += batch_loss.detach().cpu().item() * batch_importance\n",
    "        if print_loss:\n",
    "            print(f\"Total Loss: {total_loss}\")\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "63fa19ce-b445-4b1e-ac4a-6c01a1f515b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNLSTMVAE - Number of parameters: 33967696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x00000225AF1AF680>\n",
      "<built-in method size of Tensor object at 0x000002276FD63130>\n",
      "<built-in method size of Tensor object at 0x0000022770FAF270>\n",
      "<built-in method size of Tensor object at 0x000002276FD63130>\n",
      "<built-in method size of Tensor object at 0x0000022770F7DCC0>\n",
      "<built-in method size of Tensor object at 0x000002276FD63130>\n",
      "<built-in method size of Tensor object at 0x0000022770F7DCC0>\n",
      "<built-in method size of Tensor object at 0x000002276FD63130>\n",
      "<built-in method size of Tensor object at 0x0000022770F7DCC0>\n",
      "<built-in method size of Tensor object at 0x00000227720CC9A0>\n",
      "<built-in method size of Tensor object at 0x0000022770F7DCC0>\n",
      "<built-in method size of Tensor object at 0x00000227720CC9A0>\n",
      "<built-in method size of Tensor object at 0x0000022873D381D0>\n",
      "<built-in method size of Tensor object at 0x00000227720CC9A0>\n",
      "<built-in method size of Tensor object at 0x0000022873D381D0>\n",
      "<built-in method size of Tensor object at 0x0000022770F74950>\n",
      "<built-in method size of Tensor object at 0x0000022873D381D0>\n",
      "<built-in method size of Tensor object at 0x0000022770F74950>\n",
      "<built-in method size of Tensor object at 0x0000022873D381D0>\n",
      "<built-in method size of Tensor object at 0x0000022770F74950>\n",
      "<built-in method size of Tensor object at 0x00000227720C8DB0>\n",
      "<built-in method size of Tensor object at 0x0000022770F74950>\n",
      "<built-in method size of Tensor object at 0x0000022873D47E50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 1/10 [00:02<00:23,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x0000022874D55450>\n",
      "<built-in method size of Tensor object at 0x0000022770FAD7C0>\n",
      "<built-in method size of Tensor object at 0x0000022874D55450>\n",
      "<built-in method size of Tensor object at 0x0000022770FAD7C0>\n",
      "<built-in method size of Tensor object at 0x0000022770F689A0>\n",
      "<built-in method size of Tensor object at 0x0000022770FAD7C0>\n",
      "<built-in method size of Tensor object at 0x0000022770F689A0>\n",
      "<built-in method size of Tensor object at 0x0000022770FAD7C0>\n",
      "<built-in method size of Tensor object at 0x0000022770F689A0>\n",
      "<built-in method size of Tensor object at 0x0000022770FAD7C0>\n",
      "<built-in method size of Tensor object at 0x0000022770F689A0>\n",
      "<built-in method size of Tensor object at 0x0000022770FAD7C0>\n",
      "<built-in method size of Tensor object at 0x0000022770F689A0>\n",
      "<built-in method size of Tensor object at 0x0000022770FAD7C0>\n",
      "<built-in method size of Tensor object at 0x0000022770F689A0>\n",
      "<built-in method size of Tensor object at 0x00000225AF61C0E0>\n",
      "<built-in method size of Tensor object at 0x0000022770F689A0>\n",
      "<built-in method size of Tensor object at 0x00000225AF61C0E0>\n",
      "<built-in method size of Tensor object at 0x0000022770F689A0>\n",
      "<built-in method size of Tensor object at 0x00000225AF61C0E0>\n",
      "<built-in method size of Tensor object at 0x0000022770F689A0>\n",
      "<built-in method size of Tensor object at 0x00000225AF61C0E0>\n",
      "<built-in method size of Tensor object at 0x0000022770F689A0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|        | 2/10 [00:04<00:19,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x000002276FD63130>\n",
      "<built-in method size of Tensor object at 0x0000022770F689A0>\n",
      "<built-in method size of Tensor object at 0x000002276FD63130>\n",
      "<built-in method size of Tensor object at 0x0000022770F689A0>\n",
      "<built-in method size of Tensor object at 0x000002276FD63130>\n",
      "<built-in method size of Tensor object at 0x00000225AF8BC590>\n",
      "<built-in method size of Tensor object at 0x000002276FD63130>\n",
      "<built-in method size of Tensor object at 0x00000225AF8BC590>\n",
      "<built-in method size of Tensor object at 0x000002276FD63130>\n",
      "<built-in method size of Tensor object at 0x00000225AF8BC590>\n",
      "<built-in method size of Tensor object at 0x000002276FD63130>\n",
      "<built-in method size of Tensor object at 0x00000225AF8BC590>\n",
      "<built-in method size of Tensor object at 0x000002276FD63130>\n",
      "<built-in method size of Tensor object at 0x0000022770FACC20>\n",
      "<built-in method size of Tensor object at 0x000002276FD63130>\n",
      "<built-in method size of Tensor object at 0x0000022770FACC20>\n",
      "<built-in method size of Tensor object at 0x0000022874D79AE0>\n",
      "<built-in method size of Tensor object at 0x0000022770FACC20>\n",
      "<built-in method size of Tensor object at 0x0000022874D79AE0>\n",
      "<built-in method size of Tensor object at 0x0000022770F689A0>\n",
      "<built-in method size of Tensor object at 0x0000022874D79AE0>\n",
      "<built-in method size of Tensor object at 0x0000022770F689A0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|       | 3/10 [00:07<00:17,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x0000022770F630E0>\n",
      "<built-in method size of Tensor object at 0x0000022770F74950>\n",
      "<built-in method size of Tensor object at 0x00000227720C0770>\n",
      "<built-in method size of Tensor object at 0x00000225AF1FB630>\n",
      "<built-in method size of Tensor object at 0x00000227720C0770>\n",
      "<built-in method size of Tensor object at 0x00000225AF1FB630>\n",
      "<built-in method size of Tensor object at 0x00000227720C0770>\n",
      "<built-in method size of Tensor object at 0x00000227720CEE00>\n",
      "<built-in method size of Tensor object at 0x00000227720C0770>\n",
      "<built-in method size of Tensor object at 0x00000227720CEE00>\n",
      "<built-in method size of Tensor object at 0x00000227720C0770>\n",
      "<built-in method size of Tensor object at 0x00000227720CEE00>\n",
      "<built-in method size of Tensor object at 0x0000022770F74950>\n",
      "<built-in method size of Tensor object at 0x00000227720CEE00>\n",
      "<built-in method size of Tensor object at 0x0000022770F74950>\n",
      "<built-in method size of Tensor object at 0x00000227720CEE00>\n",
      "<built-in method size of Tensor object at 0x0000022770F74950>\n",
      "<built-in method size of Tensor object at 0x00000227720CEE00>\n",
      "<built-in method size of Tensor object at 0x0000022770F74950>\n",
      "<built-in method size of Tensor object at 0x00000227720CEE00>\n",
      "<built-in method size of Tensor object at 0x0000022770F74950>\n",
      "<built-in method size of Tensor object at 0x0000022770FAFE50>\n",
      "<built-in method size of Tensor object at 0x0000022770F74950>\n",
      "<built-in method size of Tensor object at 0x0000022873D47E50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|      | 4/10 [00:09<00:14,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x00000225AF1AFB30>\n",
      "<built-in method size of Tensor object at 0x000002277200EA90>\n",
      "<built-in method size of Tensor object at 0x0000022770FA8DB0>\n",
      "<built-in method size of Tensor object at 0x00000225AF1FB630>\n",
      "<built-in method size of Tensor object at 0x0000022873D47E50>\n",
      "<built-in method size of Tensor object at 0x00000225AF1FB630>\n",
      "<built-in method size of Tensor object at 0x0000022873D47E50>\n",
      "<built-in method size of Tensor object at 0x00000225AF1FB630>\n",
      "<built-in method size of Tensor object at 0x0000022873D47E50>\n",
      "<built-in method size of Tensor object at 0x00000225AF1FB630>\n",
      "<built-in method size of Tensor object at 0x0000022873D47E50>\n",
      "<built-in method size of Tensor object at 0x00000225AF1FB630>\n",
      "<built-in method size of Tensor object at 0x0000022873D47E50>\n",
      "<built-in method size of Tensor object at 0x00000225AF1FB630>\n",
      "<built-in method size of Tensor object at 0x0000022770FAC7C0>\n",
      "<built-in method size of Tensor object at 0x00000225AF1FB630>\n",
      "<built-in method size of Tensor object at 0x0000022770F7D9A0>\n",
      "<built-in method size of Tensor object at 0x0000022873D47E50>\n",
      "<built-in method size of Tensor object at 0x0000022873D381D0>\n",
      "<built-in method size of Tensor object at 0x0000022873D47E50>\n",
      "<built-in method size of Tensor object at 0x0000022770F634F0>\n",
      "<built-in method size of Tensor object at 0x0000022873D47E50>\n",
      "<built-in method size of Tensor object at 0x0000022770F634F0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|     | 5/10 [00:12<00:12,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x00000227720C8DB0>\n",
      "<built-in method size of Tensor object at 0x00000227720CEBD0>\n",
      "<built-in method size of Tensor object at 0x00000225AF1FB630>\n",
      "<built-in method size of Tensor object at 0x00000225AF3AE0E0>\n",
      "<built-in method size of Tensor object at 0x00000225AF1FB630>\n",
      "<built-in method size of Tensor object at 0x00000225AF3AE0E0>\n",
      "<built-in method size of Tensor object at 0x00000225AF1FB630>\n",
      "<built-in method size of Tensor object at 0x00000225AF3AE0E0>\n",
      "<built-in method size of Tensor object at 0x00000225AF1FB630>\n",
      "<built-in method size of Tensor object at 0x00000225AF3AE0E0>\n",
      "<built-in method size of Tensor object at 0x00000225AF1FB630>\n",
      "<built-in method size of Tensor object at 0x00000225AF3AE0E0>\n",
      "<built-in method size of Tensor object at 0x00000225AF1FB630>\n",
      "<built-in method size of Tensor object at 0x00000225AF3AE0E0>\n",
      "<built-in method size of Tensor object at 0x00000225AF1FB630>\n",
      "<built-in method size of Tensor object at 0x00000225AF3AE0E0>\n",
      "<built-in method size of Tensor object at 0x0000022770F74950>\n",
      "<built-in method size of Tensor object at 0x00000225AF3AE0E0>\n",
      "<built-in method size of Tensor object at 0x0000022770F74950>\n",
      "<built-in method size of Tensor object at 0x00000225AF3AE0E0>\n",
      "<built-in method size of Tensor object at 0x0000022770F74950>\n",
      "<built-in method size of Tensor object at 0x00000225AF3AE0E0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|    | 6/10 [00:14<00:09,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x0000022874D73770>\n",
      "<built-in method size of Tensor object at 0x00000227720CC2C0>\n",
      "<built-in method size of Tensor object at 0x0000022770F630E0>\n",
      "<built-in method size of Tensor object at 0x0000022873D381D0>\n",
      "<built-in method size of Tensor object at 0x0000022770F630E0>\n",
      "<built-in method size of Tensor object at 0x0000022873D381D0>\n",
      "<built-in method size of Tensor object at 0x0000022770F630E0>\n",
      "<built-in method size of Tensor object at 0x00000225AF858C70>\n",
      "<built-in method size of Tensor object at 0x0000022770F6A900>\n",
      "<built-in method size of Tensor object at 0x00000225AF858C70>\n",
      "<built-in method size of Tensor object at 0x0000022770F6A900>\n",
      "<built-in method size of Tensor object at 0x00000227720C8E00>\n",
      "<built-in method size of Tensor object at 0x0000022770F6A900>\n",
      "<built-in method size of Tensor object at 0x0000022770F27590>\n",
      "<built-in method size of Tensor object at 0x0000022770F6A900>\n",
      "<built-in method size of Tensor object at 0x0000022770F27590>\n",
      "<built-in method size of Tensor object at 0x0000022770F6A900>\n",
      "<built-in method size of Tensor object at 0x0000022770F27590>\n",
      "<built-in method size of Tensor object at 0x00000227720CC2C0>\n",
      "<built-in method size of Tensor object at 0x0000022770F27590>\n",
      "<built-in method size of Tensor object at 0x00000227720CC2C0>\n",
      "<built-in method size of Tensor object at 0x0000022770F27590>\n",
      "<built-in method size of Tensor object at 0x00000227720CC2C0>\n",
      "<built-in method size of Tensor object at 0x00000225AF858C70>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|   | 7/10 [00:17<00:07,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x00000225AF1AFB30>\n",
      "<built-in method size of Tensor object at 0x00000225AF858C70>\n",
      "<built-in method size of Tensor object at 0x0000022874D73770>\n",
      "<built-in method size of Tensor object at 0x00000225AF858C70>\n",
      "<built-in method size of Tensor object at 0x0000022874D73770>\n",
      "<built-in method size of Tensor object at 0x00000225AF858C70>\n",
      "<built-in method size of Tensor object at 0x0000022874D73770>\n",
      "<built-in method size of Tensor object at 0x00000225AF858C70>\n",
      "<built-in method size of Tensor object at 0x0000022874D73770>\n",
      "<built-in method size of Tensor object at 0x00000225AF858C70>\n",
      "<built-in method size of Tensor object at 0x0000022874D73770>\n",
      "<built-in method size of Tensor object at 0x00000227720CEC20>\n",
      "<built-in method size of Tensor object at 0x0000022874D73770>\n",
      "<built-in method size of Tensor object at 0x00000227720CEC20>\n",
      "<built-in method size of Tensor object at 0x0000022874D73770>\n",
      "<built-in method size of Tensor object at 0x00000227720CEC20>\n",
      "<built-in method size of Tensor object at 0x0000022874D73770>\n",
      "<built-in method size of Tensor object at 0x00000227720CEC20>\n",
      "<built-in method size of Tensor object at 0x0000022874D73770>\n",
      "<built-in method size of Tensor object at 0x00000225AF1AFB30>\n",
      "<built-in method size of Tensor object at 0x0000022874D73770>\n",
      "<built-in method size of Tensor object at 0x00000225AF1AFB30>\n",
      "<built-in method size of Tensor object at 0x0000022874D73770>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|  | 8/10 [00:19<00:04,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x00000225AF858C70>\n",
      "<built-in method size of Tensor object at 0x00000225AF61C0E0>\n",
      "<built-in method size of Tensor object at 0x00000225AF858C70>\n",
      "<built-in method size of Tensor object at 0x00000225AF61C0E0>\n",
      "<built-in method size of Tensor object at 0x00000225AF858C70>\n",
      "<built-in method size of Tensor object at 0x00000225AF61C0E0>\n",
      "<built-in method size of Tensor object at 0x00000225AF858C70>\n",
      "<built-in method size of Tensor object at 0x00000225AF61C0E0>\n",
      "<built-in method size of Tensor object at 0x00000225AF858C70>\n",
      "<built-in method size of Tensor object at 0x00000225AF61C0E0>\n",
      "<built-in method size of Tensor object at 0x00000225AF858C70>\n",
      "<built-in method size of Tensor object at 0x00000225AF61C0E0>\n",
      "<built-in method size of Tensor object at 0x00000225AF858C70>\n",
      "<built-in method size of Tensor object at 0x00000225AF61C0E0>\n",
      "<built-in method size of Tensor object at 0x00000225AF858C70>\n",
      "<built-in method size of Tensor object at 0x00000225AF61C0E0>\n",
      "<built-in method size of Tensor object at 0x00000225AF858C70>\n",
      "<built-in method size of Tensor object at 0x00000225AF61C0E0>\n",
      "<built-in method size of Tensor object at 0x00000225AF858C70>\n",
      "<built-in method size of Tensor object at 0x0000022770F63D10>\n",
      "<built-in method size of Tensor object at 0x00000225AF858C70>\n",
      "<built-in method size of Tensor object at 0x0000022770F27590>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%| | 9/10 [00:22<00:02,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x00000225AF858C70>\n",
      "<built-in method size of Tensor object at 0x00000225AF8BC590>\n",
      "<built-in method size of Tensor object at 0x0000022770FAD1D0>\n",
      "<built-in method size of Tensor object at 0x00000225AF8BC590>\n",
      "<built-in method size of Tensor object at 0x00000227720E6F40>\n",
      "<built-in method size of Tensor object at 0x00000225AF8BC590>\n",
      "<built-in method size of Tensor object at 0x00000227720E6F40>\n",
      "<built-in method size of Tensor object at 0x00000225AF8BC590>\n",
      "<built-in method size of Tensor object at 0x00000227720E6F40>\n",
      "<built-in method size of Tensor object at 0x00000225AF8BC590>\n",
      "<built-in method size of Tensor object at 0x0000022770F7D090>\n",
      "<built-in method size of Tensor object at 0x00000225AF8BC590>\n",
      "<built-in method size of Tensor object at 0x00000225AF432270>\n",
      "<built-in method size of Tensor object at 0x00000225AF8BC590>\n",
      "<built-in method size of Tensor object at 0x00000227720CEBD0>\n",
      "<built-in method size of Tensor object at 0x00000225AF8BC590>\n",
      "<built-in method size of Tensor object at 0x00000227720CEBD0>\n",
      "<built-in method size of Tensor object at 0x00000225AF8BC590>\n",
      "<built-in method size of Tensor object at 0x00000227720CEBD0>\n",
      "<built-in method size of Tensor object at 0x00000225AF8BC590>\n",
      "<built-in method size of Tensor object at 0x00000227720CEBD0>\n",
      "<built-in method size of Tensor object at 0x00000225AF8BC590>\n",
      "<built-in method size of Tensor object at 0x00000225AF1AFB30>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x00000225AF8BC590>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbsAAAGHCAYAAAAp0fzIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/OQEPoAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAziElEQVR4nO3dfVhUdf4//ucZBgYGBhCQGUmEcZePqWimGJ+whELpRi3XNW21cq+69leLtZG1FllJVrBauexKabZ9y/Iyq8+mudvNSpaYmUUo3ahltQh4M+INDiAIDPP+/THMwRFRbs7MmRmej+uahTlz5syLudrr6fu87yQhhAAREZEf06hdABERkbsx7IiIyO8x7IiIyO8x7IiIyO8x7IiIyO8x7IiIyO8x7IiIyO8x7IiIyO8x7IiIyO8x7IguQJKkbj22bt160Wvl5+dj48aNfa4nLy+vW+fde++9ffosIn+iVbsAIm/2xRdfuDx/6qmn8Omnn+KTTz5xOT5ixIiLXis/Px8zZ87E9OnTlSyRiLqBYUd0Af/7v//r8nzgwIHQaDSdjhORd+NtTKI+OnnyJLKzs3HJJZcgKCgIQ4cOxaJFi9Dc3CyfI0kSTp8+jTVr1si3PjMyMgAAx44dQ3Z2NkaMGIGwsDDExsbi2muvxWeffaZ63QDwzjvvIDU1FREREdDr9Rg6dCjuvPNO+XW73Y6nn34aw4YNQ0hICCIjIzF69Gj87W9/c2v9RD3Blh1RH5w5cwbXXHMNfvnlFzz55JMYPXo0PvvsMxQUFKC8vBzvv/8+AMft0GuvvRbXXHMNHn/8cQBAeHg4AEfoAMDixYthMpnQ0NCADRs2ICMjA1u2bJFDUa26Z8+ejdmzZyMvLw/BwcGorKx0uY27bNky5OXl4bHHHsPEiRPR2tqKH374AadOnVK8bqJeE0TUbfPmzROhoaHy81WrVgkA4u2333Y5b+nSpQKA2Lx5s3wsNDRUzJs376KfYbPZRGtrq8jMzBS/+c1vXF4DIBYvXnzRawAQ8+fP7/L17tb93HPPCQDi1KlTXV5r6tSpYsyYMRetiUhNvI1J1AeffPIJQkNDMXPmTJfjv//97wEAW7Zs6dZ1Vq1ahbFjxyI4OBharRaBgYHYsmUL9u3bp3TJALpf9/jx4wEAs2bNwttvv41Dhw51utYVV1yBb775BtnZ2fjPf/6Duro6t9RM1BcMO6I+OHHiBEwmEyRJcjkeGxsLrVaLEydOXPQay5cvxx//+Eekpqbin//8J3bu3InS0lJcf/31aGpqUrXuiRMnYuPGjbDZbLjjjjswePBgJCcn480335Tfk5ubi+eeew47d+7EDTfcgOjoaGRmZuLrr792S+1EvcGwI+qD6OhoHD16FEIIl+M1NTWw2WyIiYm56DXWrl2LjIwMrFy5ElOmTEFqaipSUlJQX1/vrrJ7VPfNN9+MLVu2wGq1YuvWrRg8eDDmzJkjT8vQarVYsGABdu3ahZMnT+LNN99EdXU1rrvuOjQ2NrrtbyDqCYYdUR9kZmaioaGh02Tx119/XX7dSafTnbelJkkSdDqdy7Fvv/220xw/JfWkbiedTof09HQsXboUALB79+5O50RGRmLmzJmYP38+Tp48iQMHDiheO1FvcDQmUR/ccccdeOGFFzBv3jwcOHAAo0aNwvbt25Gfn48bb7wRkyZNks8dNWoUtm7din/9618YNGgQDAYDhg0bhqlTp+Kpp57C4sWLkZ6ejh9//BFLliyB2WyGzWbrdW2//PIL/u///q/T8REjRnS77ieeeAIHDx5EZmYmBg8ejFOnTuFvf/sbAgMDkZ6eDgCYNm0akpOTkZKSgoEDB6KyshKFhYVISEhAUlJSr+snUpTaI2SIfMm5ozGFEOLEiRPinnvuEYMGDRJarVYkJCSI3NxccebMGZfzysvLxYQJE4RerxcARHp6uhBCiObmZvHQQw+JSy65RAQHB4uxY8eKjRs3innz5omEhASXa6AHozG7ejjf3526//3vf4sbbrhBXHLJJSIoKEjExsaKG2+8UXz22WfyOc8//7xIS0sTMTExIigoSAwZMkTcdddd4sCBA93/YoncTBLinJv2REREfoZ9dkRE5PcYdkRE5PcYdkRE5PcYdkRE5PcYdkRE5PcYdkRE5Pd8clK53W7H4cOHYTAYOq3tR0RE/YcQAvX19YiLi4NG03X7zSfD7vDhw4iPj1e7DCIi8hLV1dUYPHhwl6/7ZNgZDAYAjj/OuQEmERH1P3V1dYiPj5dzoSs+GXbOW5fh4eEMOyIiumiXFgeoEBGR32PYERGR32PYERGR32PYERGR32PYERGR3+tx2G3btg3Tpk1DXFwcJEnCxo0b5ddaW1vx8MMPY9SoUQgNDUVcXBzuuOMOHD582OUazc3NuO+++xATE4PQ0FDcdNNNOHjwYJ//GCIiovPpcdidPn0al112GYqKijq91tjYiF27duHxxx/Hrl278O6772L//v246aabXM7LycnBhg0bsH79emzfvh0NDQ2YOnUq2traev+XEBERdaFPO5VLkoQNGzZg+vTpXZ5TWlqKK664ApWVlRgyZAisVisGDhyIN954A7NnzwbQsSLKBx98gOuuu+6in1tXV4eIiAhYrVbOsyMi6se6mwdu77OzWq2QJAmRkZEAgLKyMrS2tiIrK0s+Jy4uDsnJydixY8d5r9Hc3Iy6ujqXBxERUXe5NezOnDmDRx55BHPmzJET12KxICgoCAMGDHA512g0wmKxnPc6BQUFiIiIkB9KrIvZ1NKG7T8dx3vlh/p8LSIi8m5uC7vW1lbceuutsNvtePHFFy96vhCiy+VecnNzYbVa5Ud1dXWf6zve0IzbXvkSf37nW7TZe30nl4iIfIBbwq61tRWzZs1CRUUFiouLXe6jmkwmtLS0oLa21uU9NTU1MBqN572eTqeT18FUaj3MuMgQBAVo0NJmx+FTTX2+HhEReS/Fw84ZdD/99BM+/vhjREdHu7w+btw4BAYGori4WD525MgRfP/990hLS1O6nC4FaCTER4UAAA6cOO2xzyUiIs/r8a4HDQ0N+Pnnn+XnFRUVKC8vR1RUFOLi4jBz5kzs2rUL//73v9HW1ib3w0VFRSEoKAgRERG466678OCDDyI6OhpRUVF46KGHMGrUKEyaNEm5v6wbzDGh+OXYaRw40Yirkzz60URE5EE9Druvv/4a11xzjfx8wYIFAIB58+YhLy8PmzZtAgCMGTPG5X2ffvopMjIyAAB//etfodVqMWvWLDQ1NSEzMxOvvfYaAgICevln9E5idCgA4MBxtuyIiPxZj8MuIyMDF5qa151pe8HBwVixYgVWrFjR049XVEIMw46IqD/o12tjmttbdhXssyMi8mv9OuwSY/QAgOqTjbC12VWuhoiI3KVfh11cRAiCtBq0tgkcsZ5RuxwiInKTfh12Go2EhChH666C/XZERH6rX4cdACQ4R2Sy346IyG/1+7Azx7BlR0Tk7/p92CVy+gERkd/r92HnnH5QeaJR5UqIiMhd+n3YOSeWV3H6ARGR3+r3YTcoPBg6rQY2u8Ah7n5AROSX+n3YaTQSEqI5SIWIyJ/1+7ADuCA0EZG/Y9jBsdUPABzgIBUiIr/EsAMnlhMR+TuGHToWhOZtTCIi/8SwQ8dtzOraJrRy+gERkd9h2AEwGoIRHKhBm13gYC2nHxAR+RuGHRzTDxLZb0dE5LcYdu2cc+3Yb0dE5H8Ydu24IDQRkf9i2LVzLghdwbl2RER+h2HXztmyq2SfHRGR32HYtXNOPzjI6QdERH6HYdcu1qBDSGAA2uwC1Sd5K5OIyJ8w7NpJUsfuB5x+QETkXxh2Z3Heyqw4zpYdEZE/YdidhYNUiIj8E8PuLPL0A861IyLyKwy7s7DPjojIPzHszuLssztU24QWG6cfEBH5C4bdWQYadAgNCoBdAFWcfkBE5DcYdmdxTD/gIBUiIn/DsDtHx/QDhh0Rkb9g2J2Dg1SIiPwPw+4cHVv9sM+OiMhfMOzOwduYRET+p8dht23bNkybNg1xcXGQJAkbN250eV0Igby8PMTFxSEkJAQZGRnYs2ePyznNzc247777EBMTg9DQUNx00004ePBgn/4QpSS2D1A5bG1Cs61N5WqIiEgJPQ6706dP47LLLkNRUdF5X1+2bBmWL1+OoqIilJaWwmQyYfLkyaivr5fPycnJwYYNG7B+/Xps374dDQ0NmDp1Ktra1A+XmLAghAYFQAhw9wMiIn8h+gCA2LBhg/zcbrcLk8kk/vKXv8jHzpw5IyIiIsSqVauEEEKcOnVKBAYGivXr18vnHDp0SGg0GvHRRx9163OtVqsAIKxWa1/K79KNf9smEh7+t9i8x+KW6xMRkTK6mweK9tlVVFTAYrEgKytLPqbT6ZCeno4dO3YAAMrKytDa2upyTlxcHJKTk+VzztXc3Iy6ujqXhzt1DFJhvx0RkT9QNOwsFgsAwGg0uhw3Go3yaxaLBUFBQRgwYECX55yroKAAERER8iM+Pl7JsjuRF4Tm9AMiIr/gltGYkiS5PBdCdDp2rgudk5ubC6vVKj+qq6sVq/V8uNUPEZF/UTTsTCYTAHRqodXU1MitPZPJhJaWFtTW1nZ5zrl0Oh3Cw8NdHu6U6JxYzrl2RER+QdGwM5vNMJlMKC4ulo+1tLSgpKQEaWlpAIBx48YhMDDQ5ZwjR47g+++/l89Rm7Nld9jahDOt6o8QJSKivtH29A0NDQ34+eef5ecVFRUoLy9HVFQUhgwZgpycHOTn5yMpKQlJSUnIz8+HXq/HnDlzAAARERG466678OCDDyI6OhpRUVF46KGHMGrUKEyaNEm5v6wPokODYNBpUd9sQ9XJRvyP0aB2SURE1Ac9Druvv/4a11xzjfx8wYIFAIB58+bhtddew8KFC9HU1ITs7GzU1tYiNTUVmzdvhsHQERh//etfodVqMWvWLDQ1NSEzMxOvvfYaAgICFPiT+k6SJCTGhOK7Q1YcOH6aYUdE5OMkIYRQu4ieqqurQ0REBKxWq9v67+57czf+9c1hPHrjpfj/Jv7KLZ9BRER909084NqYXXAOUqngIBUiIp/HsOuCc41MTiwnIvJ9DLsuyKuocK4dEZHPY9h1wbnVzxHrGU4/ICLycQy7LgzQByI82DFYtfIE++2IiHwZw64LzukHADdyJSLydQy7C5AHqbDfjojIpzHsLoBb/RAR+QeG3QWYY9oXhGbLjojIpzHsLqBjrh0HqBAR+TKG3QU4w85SdwZNLZx+QETkqxh2FzAgNAgRIYEAeCuTiMiXMewugoNUiIh8H8PuIszOXcs5sZyIyGcx7C6CLTsiIt/HsLsI5yCVCvbZERH5LIbdRbBlR0Tk+xh2F2Fub9nV1DfjdLNN5WqIiKg3GHYXEaEPxAC9Y/oBdz8gIvJNDLtuSOCC0EREPo1h1w1mbvVDROTTGHbd0LFGJsOOiMgXMey6IbF99wP22RER+SaGXTfItzHZZ0dE5JMYdt3gHKByrL4ZDZx+QETkcxh23RAREoio0CAA7LcjIvJFDLtuSozmruVERL6KYddNzmXDOEiFiMj3MOy6yblsGOfaERH5HoZdNyVwQWgiIp/FsOsmM5cMIyLyWQy7bnJOLD/e0IL6M60qV0NERD3BsOsmQ3AgYsIc0w84SIWIyLcw7HogkYNUiIh8EsOuBxK4IDQRkU9i2PWAub3fjmtkEhH5FsXDzmaz4bHHHoPZbEZISAiGDh2KJUuWwG63y+cIIZCXl4e4uDiEhIQgIyMDe/bsUboUxSVy+gERkU9SPOyWLl2KVatWoaioCPv27cOyZcvw7LPPYsWKFfI5y5Ytw/Lly1FUVITS0lKYTCZMnjwZ9fX1SpejKGefHQeoEBH5FsXD7osvvsDNN9+MKVOmIDExETNnzkRWVha+/vprAI5WXWFhIRYtWoQZM2YgOTkZa9asQWNjI9atW6d0OYpytuxOnG5BHacfEBH5DMXD7qqrrsKWLVuwf/9+AMA333yD7du348YbbwQAVFRUwGKxICsrS36PTqdDeno6duzYcd5rNjc3o66uzuWhhjCdFjFhOgC8lUlE5Eu0Sl/w4YcfhtVqxaWXXoqAgAC0tbXhmWeewe9+9zsAgMViAQAYjUaX9xmNRlRWVp73mgUFBXjyySeVLrVXzDF6HG9oRsXx0xg9OFLtcoiIqBsUb9m99dZbWLt2LdatW4ddu3ZhzZo1eO6557BmzRqX8yRJcnkuhOh0zCk3NxdWq1V+VFdXK112tyXK0w/Yb0dE5CsUb9n9+c9/xiOPPIJbb70VADBq1ChUVlaioKAA8+bNg8lkAuBo4Q0aNEh+X01NTafWnpNOp4NOp1O61F7p2OqHtzGJiHyF4i27xsZGaDSulw0ICJCnHpjNZphMJhQXF8uvt7S0oKSkBGlpaUqXozhze9hxrh0Rke9QvGU3bdo0PPPMMxgyZAhGjhyJ3bt3Y/ny5bjzzjsBOG5f5uTkID8/H0lJSUhKSkJ+fj70ej3mzJmjdDmKS3DuWM4BKkREPkPxsFuxYgUef/xxZGdno6amBnFxcbj77rvxxBNPyOcsXLgQTU1NyM7ORm1tLVJTU7F582YYDAaly1Gcs8+utrEV1sZWROgDVa6IiIguRhJCCLWL6Km6ujpERETAarUiPDzc459/xTMfo6a+Ge/Nn4DL4iM9/vlEROTQ3Tzg2pi9IC8bxn47IiKfwLDrhcT2fjtu9UNE5BsYdr3ABaGJiHwLw64XzM5NXLkgNBGRT2DY9QInlhMR+RaGXS8459qdamzFqcYWlashIqKLYdj1gj5IC2O4Y/kyDlIhIvJ+DLtekheE5q1MIiKvx7DrJXmNTO5+QETk9Rh2vcRBKkREvoNh10sd+9ox7IiIvB3DrpcSYzpWUfHB5UWJiPoVhl0vJUQ5WnZ1Z2yobWxVuRoiIroQhl0vhQQFYFBEMABOPyAi8nYMuz5w9ttxkAoRkXdj2PUBF4QmIvINDLs+kLf64YLQRERejWHXB2zZERH5BoZdH5jPCjtOPyAi8l4Muz4YEqWHJAH1zTacPM3dD4iIvBXDrg+CAwMQFxECgAtCExF5M4ZdHzn3tuOC0ERE3oth10ccpEJE5P0Ydn1k5r52RERej2HXR3LLjmFHROS1GHZ9ZG7f/eDA8UZOPyAi8lIMuz4aPMAx/aCh2YbjDZx+QETkjRh2fcTpB0RE3o9hpwAzR2QSEXk1hp0CnLuWs2VHROSdGHYKcO5rd4ATy4mIvBLDTgHOsOOO5URE3olhp4Cz59px+gERkfdh2ClgSJQeGglobGnDsYZmtcshIqJzMOwUEKTV4JIB7dMP2G9HROR1GHYK6Rikwn47IiJv45awO3ToEG677TZER0dDr9djzJgxKCsrk18XQiAvLw9xcXEICQlBRkYG9uzZ445SPEYepMLpB0REXkfxsKutrcWECRMQGBiIDz/8EHv37sXzzz+PyMhI+Zxly5Zh+fLlKCoqQmlpKUwmEyZPnoz6+nqly/EYbvVDROS9tEpfcOnSpYiPj8err74qH0tMTJR/F0KgsLAQixYtwowZMwAAa9asgdFoxLp163D33XcrXZJHyAtCn2CfHRGRt1G8Zbdp0yakpKTglltuQWxsLC6//HK8/PLL8usVFRWwWCzIysqSj+l0OqSnp2PHjh3nvWZzczPq6upcHt7GeRuzktMPiIi8juJh99///hcrV65EUlIS/vOf/+Cee+7Bn/70J7z++usAAIvFAgAwGo0u7zMajfJr5yooKEBERIT8iI+PV7rsPhs8oGP6QU09px8QEXkTxcPObrdj7NixyM/Px+WXX467774bf/jDH7By5UqX8yRJcnkuhOh0zCk3NxdWq1V+VFdXK112nwVpNRg8wHErkyupEBF5F8XDbtCgQRgxYoTLseHDh6OqqgoAYDKZAKBTK66mpqZTa89Jp9MhPDzc5eGNOEiFiMg7KR52EyZMwI8//uhybP/+/UhISAAAmM1mmEwmFBcXy6+3tLSgpKQEaWlpSpfjUeZoDlIhIvJGio/GfOCBB5CWlob8/HzMmjULX331FVavXo3Vq1cDcNy+zMnJQX5+PpKSkpCUlIT8/Hzo9XrMmTNH6XI8ii07IiLvpHjYjR8/Hhs2bEBubi6WLFkCs9mMwsJCzJ07Vz5n4cKFaGpqQnZ2Nmpra5GamorNmzfDYDAoXY5HyauocGI5EZFXkYQPjpOvq6tDREQErFarV/XfVRw/jWue24rgQA32Pnk9NJrzD7ghIiJldDcPuDamggYPCEGARsKZVjunHxAReRGGnYICAzSIb9/9gNMPiIi8B8NOYWdv5EpERN6BYacwbvVDROR9GHYKS4zmKipERN6GYacw523MSk4sJyLyGgw7hZnP6rOz231uVgcRkV9i2CnsksgQaDUSmm12WOrOqF0OERGBYac4bYAG8VHta2Sy346IyCsw7NxAHqTC6QdERF6BYecGHKRCRORdGHZu4BykwukHRETegWHnBgmcWE5E5FUYdm5gbg+7ypONnH5AROQFGHZuEBcZjMAACS02Ow5bm9Quh4io32PYucHZ0w84SIWISH0MOzdx3srkIBUiIvUx7NyEg1SIiLwHw85NzDHtq6hwYjkRkeoYdm7SsYkr++yIiNTGsHMT5yauVSca0cbpB0REqmLYuUlcZAiCAjRoabPj8ClOPyAiUhPDzk0CNBLio0IAsN+OiEhtDDs3kjdy5YhMIiJVMezcyNlvx0EqRETqYti5USJbdkREXoFh50byVj/ssyMiUhXDzo0S2ncsrz7ZCFubXeVqiIj6L4adG8VFhCBIq0Frm8DhU2fULoeIqN9i2LmRRiMhIYrLhhERqY1h52Ydy4Yx7IiI1MKwczN5kApHZBIRqYZh52bOQSqcfkBEpB6GnZuZObGciEh1DDs3c/bZcfoBEZF6GHZuZgoPhk6rgc0ucIi7HxARqcLtYVdQUABJkpCTkyMfE0IgLy8PcXFxCAkJQUZGBvbs2ePuUlSh0UjyGpkcpEJEpA63hl1paSlWr16N0aNHuxxftmwZli9fjqKiIpSWlsJkMmHy5Mmor693Zzmq4SAVIiJ1uS3sGhoaMHfuXLz88ssYMGCAfFwIgcLCQixatAgzZsxAcnIy1qxZg8bGRqxbt85d5ahK3uqHg1SIiFThtrCbP38+pkyZgkmTJrkcr6iogMViQVZWlnxMp9MhPT0dO3bsOO+1mpubUVdX5/LwJZxYTkSkLq07Lrp+/Xrs2rULpaWlnV6zWCwAAKPR6HLcaDSisrLyvNcrKCjAk08+qXyhHiLva8fbmEREqlC8ZVddXY37778fa9euRXBwcJfnSZLk8lwI0emYU25uLqxWq/yorq5WtGZ3S4xp3/2gtgmtnH5ARORxioddWVkZampqMG7cOGi1Wmi1WpSUlODvf/87tFqt3KJztvCcampqOrX2nHQ6HcLDw10evsRoCEZwoAZtdoGDtZx+QETkaYqHXWZmJr777juUl5fLj5SUFMydOxfl5eUYOnQoTCYTiouL5fe0tLSgpKQEaWlpSpfjFc6efsB+OyIiz1O8z85gMCA5OdnlWGhoKKKjo+XjOTk5yM/PR1JSEpKSkpCfnw+9Xo85c+YoXY7XSIwOxQ+Weke/3TC1qyEi6l/cMkDlYhYuXIimpiZkZ2ejtrYWqamp2Lx5MwwGgxrleIQ8IpODVIiIPM4jYbd161aX55IkIS8vD3l5eZ74eK+Q2D6xvIJz7YiIPI5rY3oIW3ZEROph2HmIcxWVg7WNaLFx+gERkScx7Dwk1qCDPigAduEIPCIi8hyGnYdIkoQETj8gIlIFw86D5EEqx9myIyLyJIadB3GQChGROhh2HmTmbUwiIlUw7DyIW/0QEamDYedBzt0PDtU2cfoBEZEHMew8aGCYDqHt0w+qTnKQChGRpzDsPMhl+gEHqRAReQzDzsPM7LcjIvI4hp2HOfvtGHZERJ7DsPMweRNXTiwnIvIYhp2HOacfVLDPjojIYxh2HuZs2R22NuFMa5vK1RAR9Q8MOw+LCQtCmE4Lwd0PiIg8hmHnYZIkyYNUuCA0EZFnMOxUkMi5dkREHsWwU4Ez7Co4/YCIyCMYdipwjsisZNgREXkEw04FZufEcvbZERF5BMNOBZx+QETkWQw7FUSFBsHQPv2Aux8QEbkfw04FjukHXEmFiMhTGHYq4SAVIiLPYdipxBzNieVERJ7CsFOJs2XHieVERO7HsFOJvGM5b2MSEbkdw04lzh3Lj1jPoKmF0w+IiNyJYaeSAfpAhAdrAXD6ARGRuzHsVCJJkty64/QDIiL3YtipSB6kwn47IiK3YtipKIFb/RAReQTDTkVmeRNXhh0RkTsx7FTkXBC68gQHqBARuZPiYVdQUIDx48fDYDAgNjYW06dPx48//uhyjhACeXl5iIuLQ0hICDIyMrBnzx6lS/F6zgEqljpOPyAicifFw66kpATz58/Hzp07UVxcDJvNhqysLJw+3XGrbtmyZVi+fDmKiopQWloKk8mEyZMno76+XulyvFqkPgiR+kAAHKRCROROWqUv+NFHH7k8f/XVVxEbG4uysjJMnDgRQggUFhZi0aJFmDFjBgBgzZo1MBqNWLduHe6++26lS/JqCdGhONV4CgeOn8bwQeFql0NE5Jfc3mdntVoBAFFRUQCAiooKWCwWZGVlyefodDqkp6djx44d571Gc3Mz6urqXB7+Ql4Qmi07IiK3cWvYCSGwYMECXHXVVUhOTgYAWCwWAIDRaHQ512g0yq+dq6CgABEREfIjPj7enWV7lLzVD3c/ICJyG7eG3b333otvv/0Wb775ZqfXJElyeS6E6HTMKTc3F1arVX5UV1e7pV41yKuosGVHROQ2ivfZOd13333YtGkTtm3bhsGDB8vHTSYTAEcLb9CgQfLxmpqaTq09J51OB51O565SVZXIieVERG6neMtOCIF7770X7777Lj755BOYzWaX181mM0wmE4qLi+VjLS0tKCkpQVpamtLleD1n2NXUN+N0s03laoiI/JPiLbv58+dj3bp1eO+992AwGOR+uIiICISEhECSJOTk5CA/Px9JSUlISkpCfn4+9Ho95syZo3Q5Xi9CH4gB+kDUNrai8kQjRsRxRCYRkdIUD7uVK1cCADIyMlyOv/rqq/j9738PAFi4cCGampqQnZ2N2tpapKamYvPmzTAYDEqX4xMSY0JRW3UKB06cZtgREbmB4mEnhLjoOZIkIS8vD3l5eUp/vE8yR4did9UprpFJROQmXBvTC8hb/TDsiIjcgmHnBRLaJ5ZzyTAiIvdg2HkBs7yJKyeWExG5A8POCzhvYx6rb0YDpx8QESmOYecFwoMDER0aBID9dkRE7sCw8xLstyMich+GnZfgiEwiIvdh2HkJczQHqRARuQvDzkuwZUdE5D79O+xqDwBt3jH6sWP6AcOOiEhp/TfsWk4Da24C/pEJHN2jdjXyAJXjDS2oP9OqcjVERP6l/4ZdzT7gjBU4Ug68lA6UPAu0qRcyhuBAxIQ5px+w346ISEn9N+wGpwDzvwSG3QjYW4FPnwZevhawfK9aSfJGrryVSUSkqP4bdgBgMAG3rgNm/AMIGQBYvgVWZwBbl6rSyuMgFSIi9+jfYQcAkgSMvgXI/hK4dKqjlbc1H3j5GuDItx4txTlIpYItOyIiRTHsnAxGYPZa4LevACFRgOU7R+B9WgDYWjxSgryKClt2RESKYtidTZKAUTMdfXnDpwF2G1Dyl/ZW3jdu//hETiwnInILht35hMUCs94AZr4K6KOBo987Bq988oxbW3nOPruTp1tgbeL0AyIipTDsuiJJQPIMR1/eiJsdrbxtyxwDWA7vdstHhum0GGjQAQAq2W9HRKQYht3FhA0EZr0O3PIaoI8BavYAL2cCW54CbM2Kf5xzjcwK9tsRESmGYdddI3/j6Msb+RtAtAGfPeeYjH5ol6If0zFIhf12RERKYdj1RGiMo4U363UgdCBwbB/wj0nAx08q1spz9tvxNiYRkXIYdr0x4mZHX17yTEcrb/ty4KWJwMGyPl+ac+2IiJTHsOut0Ghg5iuOuXmhscCxH4BXJgHFi4HWM72+rDz9gH12RESKYdj11fBpjr68UbMAYQc+LwReuho4+HWvLpcY4+izq21shbWR0w+IiJTAsFOCPgr47cuOdTbDjMDx/cArk4HNjwOtTT27VJAWse3TD3grk4hIGQw7JV06BcjeCYy+1dHK2/F3YNXVQPVXPboMB6kQESmLYac0fRQw4yXgd+uBMBNw4ifglSzgP4u63crjXDsiImUx7Nxl2A3A/J3AZXMACOCLImDVVUDVzou+lVv9EBEpi2HnTiEDgN+sBOa8DRgGASd+Bv7f9cBHjwItXU8aN7cPUqnggtBERIpg2HnC/1zn6MsbcxsAAex8AVg1Aaj84rynJ3D6ARGRoiQhhFC7iJ6qq6tDREQErFYrwsPD1S6nZ34qBjb9Cag/DEACUu8BMh8HgkLlU5pa2jD8iY8AAOMTB+B/jIazHmGIDtOpVDwRkXfpbh4w7NRwxuoYsLL7DcfzAWbg5heAxAnyKbeu/gI7/3vyvG+PCQtyCcBhpjAkGQ0IDw70RPVERF6DYecLfv7Y0cqrO+R4fsXdwKTFQFAobG127D/agJ9q6vGjpR77j9Zj/9EGVJ3suh9vUEQwkowGDDOGyUGYZAyDPkjroT+IiMizGHa+4ozVMfl81xrH8wGJwE1FgPnq855+utmGn2sa2sOvHj8ebcBPR+txxNr1EmXxUSEY5nIr1IChA0MRHBjghj+IiMhzGHa+5uct7a28g47n4/8ATMoDdGHderu1qRU/19TjR0tHEO4/Wo/jDeffWT1AIyEhWo9hRkN7a9BxOzQhOhSBARy3RES+wSfC7sUXX8Szzz6LI0eOYOTIkSgsLMTVV5+/RXM2vww7ADhTBxQ/AZS96niuDQZCooDgcEAX3sXPiK5f14XjRFMb9h9taG8F1uOno47bonVnbOctITBAwq8GhnW6HRofpUeARvLgl0FEdHFeH3ZvvfUWbr/9drz44ouYMGECXnrpJfzjH//A3r17MWTIkAu+12/DzumXTx2tPGtV368VGNopBEVwOJo0oThhC8bR5iAcbArEgQYtfrZKOGHToU7oUQ896kUI6qGHDVoEB2rw61hH+A0zGpAQrUeQVgOtRgNtgITAAA20mvafARK0Gg0CAyQEaDpe0wY4jjlfkySGJxH1jdeHXWpqKsaOHYuVK1fKx4YPH47p06ejoKDggu/1+7ADgDYbcKoSaK5ztPjO+9Pa9eu2ni1AfSFNIsgl/OqEHo0Ihh0S7JCA9p92SBDyTw2EAOzQQAAdx9BxDJIGkDSO0JM0kCQNJI0GkCRoJA2gcRzTSBKgCYBGkiBpHOc4z9VIGmicx9qfS5LjGhIASED7b3D80MCZsZIkH4R8Vvv/SB1v7jhD0px1Hcc5knxAOuuazl/OOkeSq3D83V1wzX/pAq91vCrkert+79lPO/8TQzrrtwv/A0RIHX/xeV3sHzBdvCy1/7dzwbcq/Y+jbl6vW2cp/g83ZbsSFC9PwQsOGZOJGFN8r9/f3TxQZZheS0sLysrK8Mgjj7gcz8rKwo4dOzqd39zcjObmjp3A6+rq3F6j6gK0QPSvev/+ttb28LP2MCzrO35vdUxqD5FaEIIWxEqnlPnbuiIAtLn3I4jIu3wX8mqfwq67VAm748ePo62tDUaj0eW40WiExWLpdH5BQQGefPJJT5XnHwICHRvMhkb3/hptNkfwnS8oW047dnYAHD+FHRCi43cIl2N2ux12YYe9zQ67vc3x3O78XUAIx0/nMWEXjp/t7xV2+zm/O94j/97+umj//I77FeKsH+KsXztuaEgQ6Di94xwJQj7t7N+Bs26GuNwYOeu94qxrQpz1lu7eSBHdPLUbJ3X75k0fzzvnsNTt6/XhMy9A6tVNq158jqJ/Z28quLBetcEuWISyFeoMUYperyuqTsA697aEEOK8typyc3OxYMEC+XldXR3i493/L4F+L0Dr2MVB3/f/GDXg2nREpB5Vwi4mJgYBAQGdWnE1NTWdWnsAoNPpoNNxiSwiIuodVf6xHRQUhHHjxqG4uNjleHFxMdLS0tQoiYiI/JhqtzEXLFiA22+/HSkpKbjyyiuxevVqVFVV4Z577lGrJCIi8lOqhd3s2bNx4sQJLFmyBEeOHEFycjI++OADJCQkqFUSERH5KS4XRkREPqu7ecABckRE5PcYdkRE5PcYdkRE5PcYdkRE5PcYdkRE5PcYdkRE5PdUXRuzt5yzJfrF7gdERNQlZw5cbBadT4ZdfX09AHAxaCIiAuDIhYiIiC5f98lJ5Xa7HYcPH4bBYOjTho7O3ROqq6s5Ob0H+L31Hr+73uH31jv94XsTQqC+vh5xcXHQaLrumfPJlp1Go8HgwYMVu154eLjf/ofgTvzeeo/fXe/we+sdf//eLtSic+IAFSIi8nsMOyIi8nv9Oux0Oh0WL17MjWF7iN9b7/G76x1+b73D762DTw5QISIi6ol+3bIjIqL+gWFHRER+j2FHRER+j2FHRER+r9+G3Ysvvgiz2Yzg4GCMGzcOn332mdoleb2CggKMHz8eBoMBsbGxmD59On788Ue1y/I5BQUFkCQJOTk5apfi9Q4dOoTbbrsN0dHR0Ov1GDNmDMrKytQuy+vZbDY89thjMJvNCAkJwdChQ7FkyRLY7Xa1S1NNvwy7t956Czk5OVi0aBF2796Nq6++GjfccAOqqqrULs2rlZSUYP78+di5cyeKi4ths9mQlZWF06dPq12azygtLcXq1asxevRotUvxerW1tZgwYQICAwPx4YcfYu/evXj++ecRGRmpdmleb+nSpVi1ahWKioqwb98+LFu2DM8++yxWrFihdmmq6ZdTD1JTUzF27FisXLlSPjZ8+HBMnz4dBQUFKlbmW44dO4bY2FiUlJRg4sSJapfj9RoaGjB27Fi8+OKLePrppzFmzBgUFhaqXZbXeuSRR/D555/zrksvTJ06FUajEa+88op87Le//S30ej3eeOMNFStTT79r2bW0tKCsrAxZWVkux7OysrBjxw6VqvJNVqsVABAVFaVyJb5h/vz5mDJlCiZNmqR2KT5h06ZNSElJwS233ILY2FhcfvnlePnll9UuyydcddVV2LJlC/bv3w8A+Oabb7B9+3bceOONKlemHp9cCLovjh8/jra2NhiNRpfjRqMRFotFpap8jxACCxYswFVXXYXk5GS1y/F669evx65du1BaWqp2KT7jv//9L1auXIkFCxbg0UcfxVdffYU//elP0Ol0uOOOO9Quz6s9/PDDsFqtuPTSSxEQEIC2tjY888wz+N3vfqd2aarpd2HndO7WQEKIPm0X1N/ce++9+Pbbb7F9+3a1S/F61dXVuP/++7F582YEBwerXY7PsNvtSElJQX5+PgDg8ssvx549e7By5UqG3UW89dZbWLt2LdatW4eRI0eivLwcOTk5iIuLw7x589QuTxX9LuxiYmIQEBDQqRVXU1PTqbVH53ffffdh06ZN2LZtm6JbLfmrsrIy1NTUYNy4cfKxtrY2bNu2DUVFRWhubkZAQICKFXqnQYMGYcSIES7Hhg8fjn/+858qVeQ7/vznP+ORRx7BrbfeCgAYNWoUKisrUVBQ0G/Drt/12QUFBWHcuHEoLi52OV5cXIy0tDSVqvINQgjce++9ePfdd/HJJ5/AbDarXZJPyMzMxHfffYfy8nL5kZKSgrlz56K8vJxB14UJEyZ0mtqyf/9+JCQkqFSR72hsbOy0kWlAQEC/nnrQ71p2ALBgwQLcfvvtSElJwZVXXonVq1ejqqoK99xzj9qlebX58+dj3bp1eO+992AwGOTWcUREBEJCQlSuznsZDIZO/ZqhoaGIjo5mf+cFPPDAA0hLS0N+fj5mzZqFr776CqtXr8bq1avVLs3rTZs2Dc888wyGDBmCkSNHYvfu3Vi+fDnuvPNOtUtTj+inXnjhBZGQkCCCgoLE2LFjRUlJidoleT0A5328+uqrapfmc9LT08X999+vdhle71//+pdITk4WOp1OXHrppWL16tVql+QT6urqxP333y+GDBkigoODxdChQ8WiRYtEc3Oz2qWppl/OsyMiov6l3/XZERFR/8OwIyIiv8ewIyIiv8ewIyIiv8ewIyIiv8ewIyIiv8ewIyIiv8ewIyIiv8ewI6JOEhMTubEs+RWGHZGbHDt2DIGBgWhsbITNZkNoaCiqqqou+J68vDxIktTpcemll3qoaiL/1C8XgibyhC+++AJjxoyBXq/Hl19+iaioKAwZMuSi7xs5ciQ+/vhjl2NaLf+vStQXbNkRucmOHTswYcIEAMD27dvl3y9Gq9XCZDK5PGJiYuTXExMT8dRTT2HOnDkICwtDXFwcVqxY4XKNqqoq3HzzzQgLC0N4eDhmzZqFo0ePupyzadMmpKSkIDg4GDExMZgxY4bL642NjbjzzjthMBgwZMgQ7jZAPo1hR6SgqqoqREZGIjIyEsuXL8dLL72EyMhIPProo9i4cSMiIyORnZ3d58959tlnMXr0aOzatQu5ubl44IEH5D0ahRCYPn06Tp48iZKSEhQXF+OXX37B7Nmz5fe///77mDFjBqZMmYLdu3djy5YtSElJcfmM559/HikpKdi9ezeys7Pxxz/+ET/88EOfaydShcq7LhD5ldbWVlFRUSG++eYbERgYKMrLy8XPP/8swsLCRElJiaioqBDHjh3r8v2LFy8WGo1GhIaGujzuuusu+ZyEhARx/fXXu7xv9uzZ4oYbbhBCCLF582YREBAgqqqq5Nf37NkjAIivvvpKCCHElVdeKebOndtlHQkJCeK2226Tn9vtdhEbGytWrlzZsy+EyEuwI4BIQVqtFomJiXj77bcxfvx4XHbZZfj8889hNBoxceLEbl1j2LBh2LRpk8sxg8Hg8vzKK6/s9Nw5enLfvn2Ij49HfHy8/PqIESMQGRmJffv2Yfz48SgvL8cf/vCHC9YxevRo+XdJkmAymVBTU9Otv4HI2zDsiBQ0cuRIVFZWorW1FXa7HWFhYbDZbLDZbAgLC0NCQgL27NlzwWsEBQXh17/+dY8/W5IkAI7bmM7fz3b28e7sLB8YGNjp+na7vcd1EXkD9tkRKeiDDz5AeXk5TCYT1q5di/LyciQnJ6OwsBDl5eX44IMPFPmcnTt3dnrunJ4wYsQIVFVVobq6Wn597969sFqtGD58OABHq23Lli2K1ELkC9iyI1JQQkICLBYLjh49iptvvhkajQZ79+7FjBkzEBcX161r2Gw2WCwWl2OSJMFoNMrPP//8cyxbtgzTp09HcXEx3nnnHbz//vsAgEmTJmH06NGYO3cuCgsLYbPZkJ2djfT0dHkQyuLFi5GZmYlf/epXuPXWW2Gz2fDhhx9i4cKFCn0TRN6FLTsihW3duhXjx49HcHAwvvzyS1xyySXdDjoA2LNnDwYNGuTySEhIcDnnwQcfRFlZGS6//HI89dRTeP7553HdddcBcATjxo0bMWDAAEycOBGTJk3C0KFD8dZbb8nvz8jIwDvvvINNmzZhzJgxuPbaa/Hll18q8wUQeSFJCCHULoKIui8xMRE5OTnIyclRuxQin8GWHRER+T2GHRER+T3exiQiIr/Hlh0REfk9hh0REfk9hh0REfk9hh0REfk9hh0REfk9hh0REfk9hh0REfk9hh0REfm9/x8WmR56r4oE6gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "trainer = VAETrainer(model=CLvae, learning_rate=1e-3 ,  batch_size=32,  epoch=10, l2=1e-5)\n",
    "# Train the VAE model with the trainer class\n",
    "history = trainer.train(train_dataset, val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb069797-f250-4e51-bb24-dc61dcf88d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNLSTMVAE - Number of parameters: 33924144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch importance: 0.03125\n",
      "train_loss before append: 0\n",
      "train_loss after append: 1\n",
      "Epoch 0, Batch 0, Batch Loss -353103.75, Epoch Loss -11034.4921875\n",
      "Batch importance: 0.03125\n",
      "train_loss before append: 1\n",
      "train_loss after append: 2\n",
      "Batch importance: 0.03125\n",
      "train_loss before append: 2\n",
      "train_loss after append: 3\n",
      "Batch importance: 0.03125\n",
      "train_loss before append: 3\n",
      "train_loss after append: 4\n",
      "Batch importance: 0.03125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|      | 1/3 [00:00<00:00,  2.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss before append: 4\n",
      "train_loss after append: 5\n",
      "Batch importance: 0.03125\n",
      "train_loss before append: 5\n",
      "train_loss after append: 6\n",
      "Batch importance: 0.03125\n",
      "train_loss before append: 6\n",
      "train_loss after append: 7\n",
      "Batch importance: 0.03125\n",
      "train_loss before append: 7\n",
      "train_loss after append: 8\n",
      "Batch importance: 0.03125\n",
      "train_loss before append: 8\n",
      "train_loss after append: 9\n",
      "Batch importance: 0.08333333333333333\n",
      "train_loss before append: 9\n",
      "train_loss after append: 10\n",
      "Epoch 0, Average Loss 0.0\n",
      "Batch importance: 0.03125\n",
      "train_loss before append: 10\n",
      "train_loss after append: 11\n",
      "Epoch 1, Batch 0, Batch Loss -386182.9375, Epoch Loss -12068.216796875\n",
      "Batch importance: 0.03125\n",
      "train_loss before append: 11\n",
      "train_loss after append: 12\n",
      "Batch importance: 0.03125\n",
      "train_loss before append: 12\n",
      "train_loss after append: 13\n",
      "Batch importance: 0.03125\n",
      "train_loss before append: 13\n",
      "train_loss after append: 14\n",
      "Batch importance: 0.03125\n",
      "train_loss before append: 14\n",
      "train_loss after append: 15\n",
      "Batch importance: 0.03125\n",
      "train_loss before append: 15\n",
      "train_loss after append: 16\n",
      "Batch importance: 0.03125\n",
      "train_loss before append: 16\n",
      "train_loss after append: 17\n",
      "Batch importance: 0.03125\n",
      "train_loss before append: 17\n",
      "train_loss after append: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|   | 2/3 [00:00<00:00,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch importance: 0.03125\n",
      "train_loss before append: 18\n",
      "train_loss after append: 19\n",
      "Batch importance: 0.08333333333333333\n",
      "train_loss before append: 19\n",
      "train_loss after append: 20\n",
      "Epoch 1, Average Loss 0.0\n",
      "Batch importance: 0.03125\n",
      "train_loss before append: 20\n",
      "train_loss after append: 21\n",
      "Epoch 2, Batch 0, Batch Loss -506970.71875, Epoch Loss -15842.8349609375\n",
      "Batch importance: 0.03125\n",
      "train_loss before append: 21\n",
      "train_loss after append: 22\n",
      "Batch importance: 0.03125\n",
      "train_loss before append: 22\n",
      "train_loss after append: 23\n",
      "Batch importance: 0.03125\n",
      "train_loss before append: 23\n",
      "train_loss after append: 24\n",
      "Batch importance: 0.03125\n",
      "train_loss before append: 24\n",
      "train_loss after append: 25\n",
      "Batch importance: 0.03125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss before append: 25\n",
      "train_loss after append: 26\n",
      "Batch importance: 0.03125\n",
      "train_loss before append: 26\n",
      "train_loss after append: 27\n",
      "Batch importance: 0.03125\n",
      "train_loss before append: 27\n",
      "train_loss after append: 28\n",
      "Batch importance: 0.03125\n",
      "train_loss before append: 28\n",
      "train_loss after append: 29\n",
      "Batch importance: 0.08333333333333333\n",
      "train_loss before append: 29\n",
      "train_loss after append: 30\n",
      "Epoch 2, Average Loss 0.0\n",
      "Training complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Assuming you have a dataset 'dataset' which is a PyTorch Dataset\n",
    "train_loader = DataLoader(data_ini, batch_size=batch_sizes, shuffle=True)\n",
    "\n",
    "# Initialize the model\n",
    "model = CLvae\n",
    "\n",
    "# Move model to GPU if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "train_loss_list = []\n",
    "#draw_curve= True\n",
    "\n",
    "# calulate the number of parameters    \n",
    "num_params = sum(item.numel() for item in model.parameters())\n",
    "print(f\"{model.__class__.__name__} - Number of parameters: {num_params}\")\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3  # Number of epochs to train for\n",
    "for epoch in tqdm(range(num_epochs),leave=False):\n",
    "#for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    overall_loss = 0.0\n",
    "    epoch_loss = 0.0\n",
    "    for batch_idx, batch in enumerate(train_loader):  # Make sure to enumerate the loader to get the batch index\n",
    "        # Get the input data and targets\n",
    "        inputs = batch['X']\n",
    "        targets = batch['y']  # For an autoencoder, this is typically the same as inputs\n",
    "        \n",
    "        # Move data to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        \n",
    "        # Forward pass\n",
    "        batch_importance = 1 / len(inputs)\n",
    "        print(f\"Batch importance: {batch_importance}\")\n",
    "        hidden = model.init_hidden(targets.shape[0])\n",
    "        model.hidden = model.init_hidden(targets.shape[0])\n",
    "        recon, mu, log_var = model(inputs)\n",
    "        \n",
    "        # Compute losses\n",
    "        batch_loss = loss_func(recon, targets, mu, log_var)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()    # Zero out any gradients from previous pass\n",
    "        batch_loss.backward(retain_graph=True)\n",
    "        optimizer.step()  # Update the weights\n",
    "        \n",
    "        epoch_loss += batch_loss.detach().cpu().item() * batch_importance  # Aggregate the loss\n",
    "        print(f\"train_loss before append: {len(train_loss_list)}\")\n",
    "\n",
    "        train_loss_list.append(epoch_loss)\n",
    "        print(f\"train_loss after append: {len(train_loss_list)}\")\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch {epoch}, Batch {batch_idx}, Batch Loss {batch_loss.item()}, Epoch Loss {epoch_loss}')\n",
    "    \n",
    "    print(f'Epoch {epoch}, Average Loss {overall_loss / len(train_loader.dataset)}')\n",
    "\n",
    "'''\n",
    "if draw_curve:\n",
    "    x_axis = np.arange(num_epochs)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5, 4))\n",
    "    ax.plot(x_axis, train_loss_list, label=\"Train\")\n",
    "    ax.set_title(\"Total Loss\")\n",
    "    ax.set_xlabel(\"# Epoch\")   \n",
    "'''\n",
    "print('Training complete')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2c8767-e95c-43de-9b04-ab4a1c89e218",
   "metadata": {},
   "source": [
    "# GNN Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba426f82-c1bb-401a-bd67-c665a1a3cfd7",
   "metadata": {},
   "source": [
    "## Resampling the original dataset for GNN classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4defb190-1e1b-41e4-9e9d-9481de2cbf19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[#Branch1]': 0,\n",
       " '[#Branch2]': 1,\n",
       " '[#C-1]': 2,\n",
       " '[#C]': 3,\n",
       " '[#N+1]': 4,\n",
       " '[#N]': 5,\n",
       " '[#O+1]': 6,\n",
       " '[=As]': 7,\n",
       " '[=B-1]': 8,\n",
       " '[=Branch1]': 9,\n",
       " '[=Branch2]': 10,\n",
       " '[=Branch3]': 11,\n",
       " '[=C-1]': 12,\n",
       " '[=C]': 13,\n",
       " '[=Fe]': 14,\n",
       " '[=N+1]': 15,\n",
       " '[=N-1]': 16,\n",
       " '[=NH1+1]': 17,\n",
       " '[=NH2+1]': 18,\n",
       " '[=N]': 19,\n",
       " '[=O+1]': 20,\n",
       " '[=OH1+1]': 21,\n",
       " '[=O]': 22,\n",
       " '[=PH1]': 23,\n",
       " '[=P]': 24,\n",
       " '[=Pd-3]': 25,\n",
       " '[=Re+1]': 26,\n",
       " '[=Ring1]': 27,\n",
       " '[=Ring2]': 28,\n",
       " '[=S+1]': 29,\n",
       " '[=S]': 30,\n",
       " '[=Sb]': 31,\n",
       " '[=Se]': 32,\n",
       " '[=W]': 33,\n",
       " '[Ac]': 34,\n",
       " '[Ag-1]': 35,\n",
       " '[Ag]': 36,\n",
       " '[AlH3-1]': 37,\n",
       " '[AlH3-3]': 38,\n",
       " '[Al]': 39,\n",
       " '[As+1]': 40,\n",
       " '[AsH1]': 41,\n",
       " '[As]': 42,\n",
       " '[Au-1]': 43,\n",
       " '[Au-3]': 44,\n",
       " '[Au]': 45,\n",
       " '[B+1]': 46,\n",
       " '[B+2]': 47,\n",
       " '[B-1]': 48,\n",
       " '[B-2]': 49,\n",
       " '[BH2-1]': 50,\n",
       " '[BH3-1]': 51,\n",
       " '[B]': 52,\n",
       " '[Bi+1]': 53,\n",
       " '[Bi]': 54,\n",
       " '[Br-1]': 55,\n",
       " '[BrH1+1]': 56,\n",
       " '[BrH2+1]': 57,\n",
       " '[Br]': 58,\n",
       " '[Branch1]': 59,\n",
       " '[Branch2]': 60,\n",
       " '[Branch3]': 61,\n",
       " '[C+1]': 62,\n",
       " '[C+]': 63,\n",
       " '[C-1]': 64,\n",
       " '[C-]': 65,\n",
       " '[CH+]': 66,\n",
       " '[CH-]': 67,\n",
       " '[CH1-1]': 68,\n",
       " '[CH2-1]': 69,\n",
       " '[C]': 70,\n",
       " '[Ca-2]': 71,\n",
       " '[Ca-4]': 72,\n",
       " '[CaH2]': 73,\n",
       " '[Cl+3]': 74,\n",
       " '[Cl-1]': 75,\n",
       " '[ClH1+1]': 76,\n",
       " '[ClH2+1]': 77,\n",
       " '[Cl]': 78,\n",
       " '[Co+2]': 79,\n",
       " '[Co-2]': 80,\n",
       " '[Co-3]': 81,\n",
       " '[Co-4]': 82,\n",
       " '[Co]': 83,\n",
       " '[Cr]': 84,\n",
       " '[Cs+1]': 85,\n",
       " '[Cu+2]': 86,\n",
       " '[Cu-1]': 87,\n",
       " '[Cu-2]': 88,\n",
       " '[Cu-3]': 89,\n",
       " '[Cu-4]': 90,\n",
       " '[Cu-5]': 91,\n",
       " '[Cu]': 92,\n",
       " '[EOS]': 93,\n",
       " '[FH1+1]': 94,\n",
       " '[F]': 95,\n",
       " '[Fe+1]': 96,\n",
       " '[Fe+2]': 97,\n",
       " '[Fe+3]': 98,\n",
       " '[Fe-1]': 99,\n",
       " '[Fe-2]': 100,\n",
       " '[Fe-3]': 101,\n",
       " '[Fe-4]': 102,\n",
       " '[Fe]': 103,\n",
       " '[Ga-1]': 104,\n",
       " '[Ga-3]': 105,\n",
       " '[GaH3]': 106,\n",
       " '[Ga]': 107,\n",
       " '[Gd+3]': 108,\n",
       " '[GeH2+1]': 109,\n",
       " '[Ge]': 110,\n",
       " '[H+1]': 111,\n",
       " '[H-1]': 112,\n",
       " '[H]': 113,\n",
       " '[Hg-1]': 114,\n",
       " '[Hg-2]': 115,\n",
       " '[Hg]': 116,\n",
       " '[Ho]': 117,\n",
       " '[I+1]': 118,\n",
       " '[I-1]': 119,\n",
       " '[I-]': 120,\n",
       " '[IH2+1]': 121,\n",
       " '[IH2]': 122,\n",
       " '[I]': 123,\n",
       " '[Ir+1]': 124,\n",
       " '[Ir+3]': 125,\n",
       " '[Ir-3]': 126,\n",
       " '[Ir-4]': 127,\n",
       " '[K+1]': 128,\n",
       " '[KH1]': 129,\n",
       " '[LiH1]': 130,\n",
       " '[Li]': 131,\n",
       " '[MgH1]': 132,\n",
       " '[MgH2]': 133,\n",
       " '[Mg]': 134,\n",
       " '[Mn+1]': 135,\n",
       " '[Mn+2]': 136,\n",
       " '[Mn+3]': 137,\n",
       " '[Mn]': 138,\n",
       " '[Mo+1]': 139,\n",
       " '[Mo+2]': 140,\n",
       " '[Mo+]': 141,\n",
       " '[Mo-1]': 142,\n",
       " '[Mo-3]': 143,\n",
       " '[Mo]': 144,\n",
       " '[N+1]': 145,\n",
       " '[N+]': 146,\n",
       " '[N-1]': 147,\n",
       " '[N-]': 148,\n",
       " '[NH+]': 149,\n",
       " '[NH1+1]': 150,\n",
       " '[NH1-1]': 151,\n",
       " '[NH1]': 152,\n",
       " '[NH2+1]': 153,\n",
       " '[N]': 154,\n",
       " '[Na+1]': 155,\n",
       " '[NaH1]': 156,\n",
       " '[Na]': 157,\n",
       " '[Ni-2]': 158,\n",
       " '[Ni-3]': 159,\n",
       " '[Ni-4]': 160,\n",
       " '[Ni]': 161,\n",
       " '[O+1]': 162,\n",
       " '[O+]': 163,\n",
       " '[O-1]': 164,\n",
       " '[O-]': 165,\n",
       " '[OH+]': 166,\n",
       " '[OH1+1]': 167,\n",
       " '[OH2+1]': 168,\n",
       " '[O]': 169,\n",
       " '[P+1]': 170,\n",
       " '[P+]': 171,\n",
       " '[P-1]': 172,\n",
       " '[P-]': 173,\n",
       " '[PH-]': 174,\n",
       " '[PH1]': 175,\n",
       " '[PH]': 176,\n",
       " '[P]': 177,\n",
       " '[Pb]': 178,\n",
       " '[Pd+2]': 179,\n",
       " '[Pd-2]': 180,\n",
       " '[Pd-3]': 181,\n",
       " '[Pd-4]': 182,\n",
       " '[Pt+2]': 183,\n",
       " '[Pt-2]': 184,\n",
       " '[Pt-3]': 185,\n",
       " '[Pt-4]': 186,\n",
       " '[Pt]': 187,\n",
       " '[Re+1]': 188,\n",
       " '[Rh+2]': 189,\n",
       " '[Rh-4]': 190,\n",
       " '[Ring1]': 191,\n",
       " '[Ring2]': 192,\n",
       " '[Ru+1]': 193,\n",
       " '[Ru+2]': 194,\n",
       " '[Ru-2]': 195,\n",
       " '[Ru-4]': 196,\n",
       " '[Ru]': 197,\n",
       " '[S+1]': 198,\n",
       " '[S+]': 199,\n",
       " '[S-1]': 200,\n",
       " '[S-2]': 201,\n",
       " '[SH+]': 202,\n",
       " '[SH1+1]': 203,\n",
       " '[SH1]': 204,\n",
       " '[SOS]': 205,\n",
       " '[S]': 206,\n",
       " '[Sb+1]': 207,\n",
       " '[Sb]': 208,\n",
       " '[Se]': 209,\n",
       " '[SiH1-1]': 210,\n",
       " '[SiH1]': 211,\n",
       " '[Si]': 212,\n",
       " '[Sn]': 213,\n",
       " '[Tb]': 214,\n",
       " '[Te]': 215,\n",
       " '[Ti-2]': 216,\n",
       " '[Ti]': 217,\n",
       " '[Tl-1]': 218,\n",
       " '[Tl-3]': 219,\n",
       " '[U+2]': 220,\n",
       " '[U-5]': 221,\n",
       " '[U]': 222,\n",
       " '[V+]': 223,\n",
       " '[V-1]': 224,\n",
       " '[W+2]': 225,\n",
       " '[W]': 226,\n",
       " '[Zn-2]': 227,\n",
       " '[Zn-3]': 228,\n",
       " '[Zn-4]': 229,\n",
       " '[Zn]': 230,\n",
       " '[Zr]': 231}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sf = pad_start_end_token(df_sf)\n",
    "vocab = {element: idx for idx, element in enumerate(np.unique(np.concatenate(padded_sf)))}\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c29ea5a-6413-44ac-8ed5-a9d814be06cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_selfies = [tokenize_selfies(selfies, vocab) for selfies in padded_sf]\n",
    "\n",
    "# Instantiate the embedding layer\n",
    "vocab_size = len(vocab)  # Size of your vocabulary\n",
    "embedding_dim = 64  # The size of each embedding vector, which you can choose\n",
    "selfies_embedding_layer = SelfiesEmbedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Convert a list of tokenized selfies to a tensor and pass it through the embedding layer\n",
    "# Pad the sequences to the same length to create a tensor\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "padded_tokenized_selfies = pad_sequence([torch.tensor(ts) for ts in tokenized_selfies],\n",
    "                                        batch_first=True,\n",
    "                                        padding_value=0)  # assuming 0 is the padding index\n",
    "\n",
    "# Get embeddings for the batch of tokenized SELFIES\n",
    "embedded_selfies = selfies_embedding_layer(padded_tokenized_selfies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da890fcf-01bc-43db-975e-d4ce55400747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41127\n",
      "torch.Size([41127, 399, 64])\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_selfies))\n",
    "print(embedded_selfies.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86db908f-86fb-459b-a7de-d02f85a31def",
   "metadata": {},
   "source": [
    "[41127, 399, 128] indicates: <br>\r\n",
    "41127: the amount of the dataset\r\n",
    "399: the largest length of selfies string; after the pad_sequence function, all strings should have the same length which is 399\r\n",
    "128: the dimension after the embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0536b88-184e-436b-8ebc-e22d0a8f48ec",
   "metadata": {},
   "source": [
    "## Transform Selfies to Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c300922-03cd-4f5a-90b5-c8fa3c2d2fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[399, 64], edge_index=[2, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def create_graph_data_object(tokenized_selfies, embedded_selfies):\n",
    "    \"\"\"\n",
    "    Creates a graph data object for each molecule.\n",
    "    \n",
    "    Parameters:\n",
    "    - tokenized_selfies: List of lists, where each sublist is a sequence of token indices for a molecule.\n",
    "    - embedded_selfies: Tensor containing embedded vectors for each token in the tokenized_selfies.\n",
    "    \n",
    "    Returns:\n",
    "    - List of Data objects, one for each molecule.\n",
    "    \"\"\"\n",
    "    graph_data_list = []\n",
    "    \n",
    "    for i, token_list in enumerate(tokenized_selfies):\n",
    "        # Get node features from embeddings\n",
    "        node_features = embedded_selfies[i]\n",
    "        \n",
    "        # Create edges\n",
    "        # Connects each node to the next, creating a path graph\n",
    "        edge_index = []\n",
    "        for j in range(len(token_list) - 1):\n",
    "            edge_index.append([j, j + 1])\n",
    "            edge_index.append([j + 1, j])  # Adding reverse edge for undirected graph\n",
    "            \n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        \n",
    "        # Create a graph data object\n",
    "        data = Data(x=node_features, edge_index=edge_index)\n",
    "        graph_data_list.append(data)\n",
    "    \n",
    "    return graph_data_list\n",
    "\n",
    "# Assume embedded_selfies is a padded tensor where each row corresponds to embeddings of a molecule\n",
    "graph_data_list = create_graph_data_object(tokenized_selfies, embedded_selfies)\n",
    "\n",
    "# Print the first graph object\n",
    "print(graph_data_list[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859ee599-c09b-4510-b331-508d89773c8d",
   "metadata": {},
   "source": [
    "## GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a4c597d-a706-4f83-a0fa-e5cc9fe0556f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity</th>\n",
       "      <th>graph_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CI</td>\n",
       "      <td>[(x, [tensor([-0.1129,  1.5156,  1.4165, -0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CI</td>\n",
       "      <td>[(x, [tensor([-0.1129,  1.5156,  1.4165, -0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CI</td>\n",
       "      <td>[(x, [tensor([-0.1129,  1.5156,  1.4165, -0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CI</td>\n",
       "      <td>[(x, [tensor([-0.1129,  1.5156,  1.4165, -0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CI</td>\n",
       "      <td>[(x, [tensor([-0.1129,  1.5156,  1.4165, -0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41122</th>\n",
       "      <td>CI</td>\n",
       "      <td>[(x, [tensor([-0.1129,  1.5156,  1.4165, -0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41123</th>\n",
       "      <td>CI</td>\n",
       "      <td>[(x, [tensor([-0.1129,  1.5156,  1.4165, -0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41124</th>\n",
       "      <td>CI</td>\n",
       "      <td>[(x, [tensor([-0.1129,  1.5156,  1.4165, -0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41125</th>\n",
       "      <td>CI</td>\n",
       "      <td>[(x, [tensor([-0.1129,  1.5156,  1.4165, -0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41126</th>\n",
       "      <td>CI</td>\n",
       "      <td>[(x, [tensor([-0.1129,  1.5156,  1.4165, -0.02...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41127 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      activity                                         graph_data\n",
       "0           CI  [(x, [tensor([-0.1129,  1.5156,  1.4165, -0.02...\n",
       "1           CI  [(x, [tensor([-0.1129,  1.5156,  1.4165, -0.02...\n",
       "2           CI  [(x, [tensor([-0.1129,  1.5156,  1.4165, -0.02...\n",
       "3           CI  [(x, [tensor([-0.1129,  1.5156,  1.4165, -0.02...\n",
       "4           CI  [(x, [tensor([-0.1129,  1.5156,  1.4165, -0.02...\n",
       "...        ...                                                ...\n",
       "41122       CI  [(x, [tensor([-0.1129,  1.5156,  1.4165, -0.02...\n",
       "41123       CI  [(x, [tensor([-0.1129,  1.5156,  1.4165, -0.02...\n",
       "41124       CI  [(x, [tensor([-0.1129,  1.5156,  1.4165, -0.02...\n",
       "41125       CI  [(x, [tensor([-0.1129,  1.5156,  1.4165, -0.02...\n",
       "41126       CI  [(x, [tensor([-0.1129,  1.5156,  1.4165, -0.02...\n",
       "\n",
       "[41127 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure that your dataframe and your graph data list are aligned\n",
    "df_sf['graph_data'] = graph_data_list\n",
    "\n",
    "# Check that the lengths and indices are correctly aligned\n",
    "assert len(df_sf) == len(graph_data_list), \"Dataframe and graph list length mismatch.\"\n",
    "\n",
    "# Filter out any potential NaNs if something went wrong during graph creation\n",
    "df_sf = df_sf.dropna(subset=['graph_data'])\n",
    "df_gf = df_sf.drop(columns=['selfies','HIV_active'])\n",
    "df_gf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "273c2825-2c7b-4338-8a52-521cad93875b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling from each category\n",
    "ci_sample = df_gf[df_gf['activity'] == 'CI'].sample(n=1000, random_state=42)\n",
    "ca_sample = df_gf[df_gf['activity'] == 'CA'].sample(n=300, random_state=42)\n",
    "cm_sample = df_gf[df_gf['activity'] == 'CM'].sample(n=700, random_state=42)\n",
    "# Combine the samples\n",
    "graph_df = pd.concat([ci_sample, ca_sample, cm_sample])\n",
    "\n",
    "# Shuffle the combined dataframe to randomize the order of data points\n",
    "graph_df = graph_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Extract the graph data objects for training\n",
    "train_graphs = graph_df['graph_data'].tolist()\n",
    "train_labels = graph_df['activity'].tolist()  # Assuming you want to use string labels directly\n",
    "\n",
    "# If numerical labels are needed, convert them\n",
    "activity_to_index = {'CI': 0, 'CA': 1, 'CM': 2}\n",
    "train_labels = [activity_to_index[label] for label in train_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45604f1f-515f-408b-bb94-c66d7a939afb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C242",
   "language": "python",
   "name": "c242"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
